

\documentclass[final]{cmpreport_02}

\usepackage{rotating}
\usepackage{listings}
\lstset{
    basicstyle=\ttfamily,
    keywordstyle=\color{blue},
    commentstyle=\color{green!60!black},
    stringstyle=\color{red},
    showstringspaces=false,
    numbers=left,
    numberstyle=\tiny,
    numbersep=5pt,
    frame=single,
    breaklines=true
}
\usepackage{tabularx}
\usepackage{ragged2e}
\usepackage{array}
\usepackage{listings}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}
\usepackage{graphicx} % For actual images
\usepackage{longtable} % Already in your main document
\usepackage{enumitem}
\title{Jaws: Jaws Awesomely Wrangles Scheme}
\author{Jamie Michael Wales}
\registration{100067069}
\supervisor{Dr. Rudy Lapeer}
\ccode{CMP-6013Y}

\newcommand{\qbe}{\texttt{QBE}}
\newcommand{\anf}{\texttt{ANF}}
\newcommand{\tac}{\texttt{TAC}}
\newcommand{\astnode}[1]{\texttt{#1}}
\newcommand{\irnode}[1]{\texttt{ir::#1}}
\newcommand{\tacnode}[1]{\texttt{tac::#1}}
\summary{

This project, "Jaws", tackles the perceived difficulty of creating tools for functional programming languages by developing a complete version of the Scheme language that adheres to the R7RS standard. Its main aim was to build both a program that can run Scheme code directly, an interpreter and another that converts Scheme code into efficient instructions a CPU can execute, a compiler. This work demonstrates a clear path from the theoretical ideas behind Scheme to practical, working software.
The interpreter was developed using established methods to correctly read, understand, and execute Scheme programs. It carefully handles Scheme's unique characteristics, including its powerful features for extending the language, and ensures that programs run efficiently.
The compiler translates Scheme code into CPU instructions through a series of transformations. It begins by restructuring the Scheme code into a format that allows for improvements, such as performing some calculations in advance and removing unneeded code sections. This refined code is then further simplified before being transformed into an intermediate code format (QBE), which is well-suited for generating efficient programs on different CPU architectures. The resulting compiled programs are supported by a custom system written in C, which includes automatic memory management to handle memory usage effectively.
The project successfully produced a working interpreter that fully complies with the R7RS Scheme standard. This was confirmed by testing it against Chez Scheme, a well-known Scheme implementation, using a standard set of tests. The interpreter proved capable of running complex programs, including those that perform multiple tasks simultaneously, communicate over networks, or interact with code written in other programming languages. Furthermore, the compiler successfully converts Scheme code through the planned intermediate stages to the QBE format.
In conclusion, "Jaws" successfully demonstrates how a functional programming language can be built, connecting abstract concepts with practical software engineering. 
}
\acknowledgements{
Thank you to Dr. Rudy Lapeer, who helped channel my enthusiasum into a well rounded academic project. Thank you to Ella my partner who is the glue that holds my life together. To my brother, who will always listen to me go on a tangent about compilers and to my parents for their love and support.
}

\begin{document}

\section{Introduction}

Compiler construction presents a unique challenge in computer science, requiring both theoretical depth and practical engineering skill. As Cooper and Torczon \cite{cooper2011engineering} observe, this blend of theory and practice is unparalleled in the discipline. This project explores this duality through the lens of Scheme, a functional programming language that embodies elegant mathematical principles while demanding robust implementation techniques.\newline

The academic landscape of compiler development is often perceived as forbidding terrain. Ghuloum captures this sentiment perfectly: "The novice compiler writer stands puzzled facing an impenetrable barrier, 'better write an interpreter instead'" \cite{ghuloum2006incremental}. Consequently, many avoid direct compilation to machine code, favouring interpreters or bytecode virtual machines â€“ an approach popularised by Java \cite{oracle2024java} and adopted by numerous Scheme implementations \cite{SchemeImplementations}.\newline

This project deliberately challenges that perception by charting a direct course from abstract mathematics to concrete machine code for the Scheme language. Beginning with the theoretical underpinnings in Church's lambda calculus \cite{church1936unsolvable} and McCarthy's foundational work on Lisp \cite{mccarthy1960recursive}, this project specifically aims to:
\begin{itemize}[noitemsep]
    \item Develop a comprehensive R7RS-compliant interpreter for the Scheme programming language, capable of executing non-trivial programs and validated against established implementations.
    \item Construct a native code compiler for Scheme, translating high-level functional code through a multi-stage pipeline (AST $\rightarrow$ ANF $\rightarrow$ TAC) to the QBE intermediate language, targeting the x86-64 architecture and interfacing with a C runtime featuring automatic memory management. \footnote{For definitions of abbreviations please see the glossary \ref{Glossary}  }
    \item Demystify the process of functional language implementation by charting and documenting a clear pathway from Scheme's theoretical foundations and formal specifications to efficient machine code execution.
\end{itemize}



The dissertation follows this journey through the development of a complete Scheme implementation conforming to the R7RS specification \cite{r7rs}. We begin by grounding the work in mathematical foundations of lambda calculus, recursive function theory, and formal semantics. These principles inform the design of an initial interpreter that captures Scheme's semantics. This interpreter then serves as a stepping stone towards a native code compiler, evolving through a series of intermediate representations (IR's). Key transformations involve converting Scheme code into A-Normal Form (ANF) \cite{flanagan1993essence} to make evaluation order explicit, followed by further stages leading to the QBE Intermediate Language \cite{qbe_il}. QBE provides a suitable abstraction layer for generating efficient x86-64 machine code, chosen for its balance between low-level control and cross-platform potential. Foundational parsing techniques draw inspiration from established compiler literature \cite{aho2006compilers}, adapted for Scheme's unique homoiconic structure.\newline

This roadmap aims not only to demonstrate \emph{how} to implement a functional language but also to reveal the fascinating interplay between abstract theory and concrete engineering. By following this path from mathematics through interpretation and compilation to machine code, we seek to demystify the compiler construction process and show how theoretical concepts are used in a practical engineering context.\footnote{The Jaws Interpreter Website and its source code are publicly available at: \url{https://jamie-wales.github.io/Jaws}} \newline

\section{Background}
Building a compiler requires integrating deep theoretical knowledge with practical engineering skills \cite{grune2012modern}. This project delves into both aspects by exploring the implementation of Scheme, a language whose elegance stems from strong theoretical roots but whose practical realisation presents interesting engineering challenges. To fully appreciate Scheme as a compilation target and understand the design choices in this work, it is essential to examine two key areas: the fundamental principles of functional programming and the historical context provided by Lisp and its descendant, Scheme.

\subsection{Functional Programming Foundations}

Functional programming is a programming paradigm that models computation on mathematical foundations, specifically lambda calculus as formalised by Church \cite{church1936unsolvable}. It uses lambda calculus as the basis for its computation model, where a program can be considered based on its inputs and outputs. Computation proceeds through a series of steps, where an input is propagated through a program, contrasting with traditional approaches that rely on mutable state and object-oriented abstraction.\newline

As Hickey, the developer of the popular functional programming language Clojure, argues in \cite{hickey2009there}, object-oriented approaches to separation of concerns is "still a mess, it's just that particular objects mess." This highlights how functional programming provides mathematical foundations and immutability at its core, making it easier to reason about programs.\newline

Key principles of modern functional programming include:
\begin{itemize}[noitemsep]
    \item Pure functions that produce the same output for the same input without side effects
    \item Immutable data structures that cannot be changed after creation
    \item First-class functions that can be passed as arguments or returned as values
    \item Higher-order functions that operate on other functions
\end{itemize}

Languages like Haskell enforce these principles strictly, while others like Scheme and ML provide them as preferred tools while allowing imperative features when needed. These principles enable powerful abstractions for handling complexity. Pattern matching, algebraic data types, and monadic composition allow programmers to express complex operations clearly and compose them reliably. The emphasis on immutability and pure functions makes concurrent programming more manageable, as shared state, which is a major source of bugs in concurrent systems, is avoided by design.\newline

While these abstractions provide elegant ways to express computation, implementing them efficiently on real hardware presents significant challenges. Modern CPUs are designed around mutable state and sequential operations \cite{patterson2016computer}, making the compilation of functional concepts like immutability and higher-order functions non-trivial. The gap between mathematical elegance and practical realisation has existed since the early days of functional programming, where pioneers had to bridge lambda calculus's abstract notions with the realities of computer architecture. These implementation challenges, from closure representation to efficient recursion, remain relevant for modern compiler writers, making the history of how early languages like Lisp solved them particularly instructive. These factors make functional language implementation an interesting problem space for this project.
\subsection{Lisp and Scheme}
Church's work provides the proper context within functional programming, as it forms the DNA of these programming paradigms. However, it does not translate practically; it has no basis in actual computing. While lambda calculus provides the theoretical foundation, Lisp (List Processing) represents the first major practical realisation.\newline

McCarthy used Church's work to define a language to abstract computation in his seminal paper "Recursive Functions of Symbolic Expressions and Their Computation by Machine, Part I" \cite{mccarthy1960recursive}. It successfully translated the mathematical concepts of Church into a concrete implementation, and its influence can be seen in the DNA of any modern functional programming language.\newline

McCarthy introduced S-expressions as a way to represent both data and programs, demonstrating how complex data can be built from pairs. This approach supports atomic symbols and constructed expressions through list structures. The paper explains the \texttt{eval} function which serves as an interpreter; along with the homoiconic structure of Lisp, this provides the beginnings of Lisp's metacircular evaluation and metaprogramming capabilities.\newline

The paper establishes an approach to defining recursive functions, explaining how control flow and base cases can ensure termination. This approach remains clear today in recursive algorithm design. The paper also outlines approaches to memory management that are still relevant: free storage lists for dynamic memory allocation, as well as garbage collection for automatic memory management. It explains a linked list structure for efficient memory use, an approach many functional languages still use today for list implementation.\newline

Lisp's influence extends far beyond functional programming languages. Its fundamental concepts have become part of modern programming practice:
\begin{itemize}[noitemsep]
    \item Garbage collection, first introduced by McCarthy, is now standard in languages from Java to Python.
    \item Lisp's treatment of functions as first-class values appears in JavaScript's callbacks, Python's decorators, and Rust's closures.
    \item The concept of code-as-data revolutionised how we think about metaprogramming - modern build tools, domain-specific languages, and code generation all echo Lisp's homoiconic approach where programs can manipulate other programs.
\end{itemize}

However, McCarthy's paper has some limitations. While it establishes fundamental concepts, it is limited in its usefulness when looking for applied approaches. It is steeped in heavy mathematical formalism, unsurprising given McCarthy's background as a mathematician. This emphasis on lambda calculus and recursive function theory can obscure practical implementation strategies.\newline

Modern compilers need to address a wide range of problems not defined within this paper, unsurprising given its age. The paper's age also affects its immediate applicability. It explains implementation details for the IBM 704, which has little relevance to modern CPU architectures. McCarthy's memory concerns are a product of his time; we face different performance considerations and memory constraints than those of the 1960s.\newline

Nevertheless, McCarthy's work remains seminal and extremely relevant to this project. It provides methodologies and formal reasoning for language concepts and well-defined procedures such as \texttt{car}, \texttt{cdr}, and others. These functions return the head and tail of a list structure which is Scheme's fundamental data type. These operations, while named for IBM 704 assembly instructions (Contents of Address Register and Contents of Decrement Register), established patterns for list manipulation that persist in modern functional languages. Haskell's \texttt{head} and \texttt{tail}, ML's pattern matching, and even Rust's iterators all echo these fundamental list operations.\newline

The Scheme Programming Language itself, born out of the MIT Artificial Intelligence Laboratory \cite{SchemeInterpMIT}, is a dialect of Lisp \cite{structureandinterp}. As one of the foundational functional programming languages, it refined many of McCarthy's ideas while adding powerful features and emphasising lexical scoping and first-class procedures. Its minimalist design philosophy allows for a small core language, yet it remains highly expressive, partly due to its powerful macro system. The Jaws implementation follows the R7RS specification \cite{r7rs}, ensuring compatibility with the contemporary standard of the broader Scheme ecosystem.\newline

McCarthy's work directly shapes the Jaws implementation's approach. His S-expressions guide our parser design, while his \texttt{eval} function models our interpreter. Like McCarthy, we build complex features from simple primitives, starting with basic arithmetic and gradually adding closures and tail calls. This keeps the implementation both correct and manageable.

\subsection{Compiler Construction Approaches}

While direct compilation to native machine code offers potentially the highest performance, alternative implementation strategies are common, particularly in educational settings or for languages with highly dynamic features. One prevalent approach involves compiling the source language to an intermediate bytecode, which is then executed by a virtual machine (VM). This strategy was famously successful for Java \cite{oracle2024java} and is also employed by several Scheme implementations \cite{SchemeImplementations}. Bytecode VMs can simplify the compilation process by targeting a stable, abstract machine instruction set and can aid portability, but often introduce a layer of interpretation overhead compared to native code.\newline

This project, however, pursues the more direct path of native code compilation via an intermediate representation (QBE IL) to target x86-64, aiming to explore the challenges and potential benefits of generating efficient machine code directly. Successfully navigating this path requires drawing upon foundational theoretical works such as McCarthy \cite{mccarthy1960recursive} and SICP \cite{structureandinterp}, core specifications such as R7RS \cite{r7rs}), and leveraging practical implementation guides that bridge abstract requirements with concrete coding patterns. Resources such as Wilson's unpublished manuscript \cite{wilson1996introduction} "An Introduction to Scheme and its Implementation" and Ghuloum's "An incremental approach to compiler construction" \cite{ghuloum2006incremental} serve this purpose. While incomplete in some areas these types of resources offer valuable visual blueprints and code examples for complex aspects like memory layout, object representation, runtime system structure, and the implementation of features like macros and first-class procedures. They provide proven patterns that help translate the formal specifications and theoretical concepts into a working system, complementing lower-level guides focused purely on code generation.
\newline

The preceding review of functional programming foundations, the history of Lisp and Scheme, and common compiler construction techniques underscores the rich theoretical heritage and practical considerations influencing this project. Key insights identified include the power of lambda calculus as a computational model, the relevance of Lisp's core concepts like S-expressions and garbage collection, and the established multi-stage approach common in compiler design. This body of knowledge highlights both the elegant abstractions offered by functional languages and the engineering challenges in their efficient implementation, particularly in guaranteeing features like tail call optimisation and managing dynamic memory. These foundational understandings directly inform the methodological choices detailed in the subsequent sections, guiding the design of both the Jaws interpreter and its native code compiler.
\section{Methodology}
The way in which this project was approached was in three seperate sections. First, develop a interpreter for the Scheme Programming Language. Second, develop a compiler for the Scheme Programing Language. Third, develop a web-based interface for the Scheme Programming Language. \footnote{The Jaws Interpreter Website and its source code are publicly available at: \url{https://jamie-wales.github.io/Jaws}} \newline

From the outset it was clear that there was a huge amount of work to be done in order to complete the project. To give the project the best chance of developing every part a standard tree-walk interpreter approach was selected for the interpreter.\newline

A tree-walk interpreter is the simplest of the approaches to interpretation where we walk an abstract syntax tree evaluating it as we process it. This approach was selected as it is the simplest to implement and would allow for the development of the compiler and web-based interface to be developed thereafter.
A compiler typically shares its front-end with an interpreter. However, the compiler's pipeline then branches to generate an Intermediate Representation (IR), which is then converted into machine code. 

\subsection{Interpreter}

\subsubsection{Lexer}
The initial development of any compiler/interpreter project begins with establishing an effective front-end strategy. This critical first phase involves selecting appropriate approaches for lexing and parsing to transform raw text into a structured format.
For lexical analysis, several methodologies were considered, including, manual character-by-character scanning with explicit state management, using lexer generator tools like Flex or ANTL \cite{parr2013definitive}. Or finally, regular expression-based pattern matching \newline

The regular expression approach was selected for several key reasons. First, regular expressions naturally map to the deterministic finite automaton (DFA) model described by Cooper and Torczon \cite{cooper2011engineering} for lexical analysis. Second, they offer a more concise and maintainable way to express token patterns compared to manual character processing. Third, they avoid the additional learning curve and external dependencies associated with lexer generators, keeping the project self-contained.
This methodology involves organising token patterns in a prioritised list, with more specific patterns (like keywords) checked before more general ones.\newline

This prioritisation is crucial for correct tokenisation since patterns may overlap. For instance, scheme-specific patterns for complex numbers needs to be checked before simpler numeric patterns to ensure proper recognition.
For keyword handling, a two-stage approach was selected; first identify all text as a general identifier, then check a keyword map to determine if it's actually a reserved word. This approach simplifies the regex patterns while maintaining the language's semantics.
The complete set of token patterns and their priorities were determined by analysing the R7RS Scheme specification \cite{r7rs}, ensuring comprehensive coverage of the language syntax.

\subsubsection{Parser}
Parsing naturally follows lexing in the construction of an interpreter. The academic literature on parsing strategies is extensive, \cite{aho2006compilers} dedicating significant coverage to the topic. 
As with lexing, there exists tools that generate parsers automatically, typically producing table-based implementations that would be prohibitively complex to write manually.\newline 

For this project, a recursive descent parsing Algorithm approach was selected, leveraging Scheme's homoiconic nature. 
However, much of the literature on parsing optimisation originates from an era when computational efficiency was more constrained. Modern hardware has somewhat diminished these concerns relative to implementation simplicity and maintainability.
Furthermore, Scheme's homoiconic syntax, where every operation is uniformly expressed with parentheses in prefix notation, means there is no need for precedence based parsing which significantly simplifies the parsing challenge. Ultimately, it eliminates the need for complex precedence handling, making recursive descent parsing a straightforward and appropriate methodology for this project. 
(The general parsing algorithm structure is outlined in the "Recurisve Descent Parser for Scheme" algorithm in "Core Algorithms" in Appendix \ref{alg:recursive-descent}).

\subsubsection{Macros}
While most programming languages proceed directly from parsing to interpretation, Scheme introduces a crucial intermediate step, macro expansion. Scheme's powerful macro system represents one of its most distinctive features, enabling sophisticated syntactic extensions that transform the Abstract Syntax Tree before evaluation begins.

Unlike traditional C-style preprocessor macros that operate through simple text substitution, Scheme implements a hygienic macro system that works with structured expressions. This methodological approach provides significant advantages:

\begin{itemize}[noitemsep]
    \item Elimination of variable capture problems and unintended evaluation order issues
    \item Pattern-matching capabilities that recognise and transform complex syntax patterns
    \item Preservation of lexical scoping during expansion
    \item The ability to create domain-specific language extensions within Scheme itself
\end{itemize}

The implementation for this project follows the R7RS specification's syntax-rules system, which defines macros as pairs of patterns and templates. When a macro is invoked, the system matches the invocation against defined patterns sequentially until finding a match, then generates the corresponding template with appropriate substitution.\newline

To ensure proper hygiene, the implementation required a separate evaluation environment for macro expansion. This methodology involves tracking the origin of each identifier and performing appropriate renaming during expansion, preventing variables in the macro definition from capturing variables at the macro use site.
The macro expansion process transforms the initial AST into an expanded form where all macros have been replaced with their expansions. This approach aligns with the classic Scheme philosophy.

"Programming languages should be designed not by piling feature on top of feature, but by removing the weaknesses and restrictions that make additional features appear necessary." \cite{r7rs}

Enuring a minimal core language with powerful syntactic extensions built on top, effectively creating an extended language that is expressed in terms of simpler core constructs.
\subsubsection{Evaluation}
Once an AST has been developed and processed through macro expansion, the evaluation phase begins. This phase is where the Scheme language's semantics are applied to execute the program. The R7RS Scheme specification defines several key requirements that any compliant implementation must satisfy.\newline

Scheme's evaluation model requires lexical scoping, where variables are resolved in the environment where they were defined, not where they are used. This contrasts with dynamic scoping found in some other languages and necessitates a hierarchical environment structure. Additionally, Scheme treats procedures as first-class values that can be passed as arguments, returned from functions, and stored in data structures, requiring the evaluation system to handle functions as ordinary values.\newline

The language specification also defines special forms such as \texttt{if}, \texttt{lambda}, and \texttt{define} that follow unique evaluation rules different from regular function calls. To clarify why define must be a special form in Scheme, if define were implemented as a regular procedure call, the interpreter would evaluate its arguments before applying the procedure. This means following standard applicative-order evaluation would cause the interpreter to evaluate the variable name before it's defined. This would lead to errors or incorrect bindings. Additionally, the environment in which definitions occur is crucial, define must create bindings in the current lexical environment rather than in the evaluation environment of a procedure call. These requirements necessitate that define be implemented as a special form with its own unique evaluation rules that differ from regular procedure calls. This ensures proper manipulation of the environment structure according to Scheme's lexical scoping rules.\newline

Another example, conditional expressions only evaluate the necessary branches, and lambda expressions create closures without evaluating their bodies. Perhaps most importantly, the R7RS specification mandates tail call optimisation, requiring implementations to optimise tail calls to prevent stack overflow in recursive procedures. This optimisation allows recursive procedures to execute in constant space when the recursion occurs in tail position.\newline

The evaluation process uses a dispatch mechanism that selects the appropriate evaluation strategy based on the node type. Self-evaluating forms like numbers and strings simply return their value. Variable references are resolved by searching the environment chain. Special forms need to be handled by dedicated evaluation functions that implement their specific rules.\newline

The requirement for tail call optimisation presented a significant challenge, particularly when implementing in a host language that doesn't natively support this feature. To address this, a technique known as "trampolining" was selected (for an example see Algorithm~\ref{alg:trampoline} in the Appendix). When a procedure call is identified as being in tail position (the final expression to be evaluated in a function body or conditional branch), instead of executing the call directly, the evaluator signals their is a tail call storing the procedure and arguments. The main evaluation loop detects a tail call and executes them iteratively without growing the call stack. This effectively transforms recursive processes into iterative ones at the implementation level.

\subsubsection{Values}
During interpretation objects must be created to represent the values that are being manipulated. These objects need to represent the many different types as per the RSR7 specification. These objects are as follows:
\begin{itemize}[noitemsep]
    \item \textbf{Number}: Scheme's numeric tower includes integers, rationals, and floating-point numbers. The interpreter must support all these types and provide appropriate arithmetic operations for each.
    \item \textbf{String}: Scheme strings are sequences of characters enclosed in double quotes. The interpreter must provide operations for creating, accessing, and manipulating strings.
    \item \textbf{Character}: Scheme has a distinct character type for representing individual characters. The interpreter must provide operations for creating, comparing, and manipulating characters.
    \item \textbf{Symbol}: Symbols are unique identifiers used to represent variables, functions, and other named entities in Scheme. The interpreter must provide operations for creating, comparing, and interning symbols.
    \item \textbf{Boolean}: Boolean values in Scheme are \texttt{\#t} for true and \texttt{\#f} for false. The interpreter must provide operations for creating, comparing, and manipulating boolean values.
    \item \textbf{Pair}: The fundamental data structure in Scheme, pairs are used to construct lists and other compound data structures. The interpreter must provide operations for creating, accessing, and manipulating pairs.
    \item \textbf{Vector} Vectors are fixed-length sequences of elements that can contain any type of value. The interpreter must provide operations for creating, accessing, and manipulating vectors. This includes bytevectors which are vectors of bytes.
    \item \textbf{Procedure} Procedures in Scheme are first-class values, the interpreter needs to treat these as such. Procedures need to store their own environment of symbols and values. As well as constructing closures, which are procedures that return procedures, that have access to their parents scope.
    \item \textbf{Port}: Ports are Scheme's abstraction around input and output. The interpreter must provide operations for creating, reading, and writing to ports.
    \item \textbf{Continuations}: Continuations are a powerful feature of Scheme that allow for the capture and reification of the current state of the program. The interpreter must provide operations for creating, capturing, and invoking continuations.
\end{itemize}

Scheme is dynamically typed, which means an objects type is determined at runtime. Therefore the interpreter must be able to handle the many different types of objects that can be created. 
Approaches on how to handle this within the host language differ \cite{SchemeImplementations}. \newline

There are three main approaches, these are bloated structs, tagged unions/polymorphic variants, and tagged pointers. 
Bloated structs refer to a structure that holds all the possible values that an object can hold. This is the most memory intensive approach but is the fastest. Tagged unions are a more memory efficient approach but are slower than bloated structs. Tagged pointers are the most memory efficient approach but are the slowest.
During implementation polymorphic variants were selected as the approach to handle the many different types of objects that can be created, whilst maintaining type safety and satisfying the host languages type system. 
This approach was selected as it is middle ground between tagged pointers and bloated structs, and makes implementation easier and more maintainable.

\subsection{Compiler}
A major part of this project was working toward a compiler for the Scheme programming language. Modern compilers are complex programs in their own right. In order to understand the complexity of these programs, two intermediate steps were delivered.
First was transforming the AST into an IR. This IR is a data structure that represents the program in a way that is easier to manipulate and transform. The second was to perform performance optimisations on these representations.
The IR is the core data structure in a compiler, serving as the central model of the program being compiled. Most compiler components interact with this IR, reading from it and modifying it. Therefore, the design choices about what information to include in the IR, and how to structure it, significantly impact both compilation efficiency and the quality of the resulting code \cite{cooper2011engineering}. 
The IRs chosen for the project were A Normal Form (ANF) and Three Address Code (TAC). For ANF the optimisation chosen is dead code elimination and constant folding. For TAC the optimisation chosen was scope hoisting and inline definition.

\subsubsection{A Normal Form}
Administrative Normal Form \cite{flanagan1993essence} is a compiler intermediate representation where every non-trivial expression is assigned to a variable, ensuring that function arguments are always simple atomic values (like variables or constants) rather than complex expressions. This transformation makes the evaluation order explicit and simplifies subsequent compiler optimisations by breaking complex nested expressions into a sequence of simpler statements, similar to how statements work in imperative languages but preserving functional semantics.\newline

There are two compelling reasons ANF was selected as the first intermediate representation and is chosen for many functional programming language compilers. First, ANF naturally corresponds to Scheme's fundamental structure, as Scheme programs can be expressed as nested let-bindings, making ANF transformations particularly elegant. Second, ANF serves as an ideal bridge between high-level functional code and lower-level representations, facilitating subsequent optimisation passes by exposing the program's evaluation order while maintaining its pure functional semantics.

\subsubsection{Dead Code Elimination}
Dead code elimination (DCE) integrates seamlessly with ANF due to the clear execution flow that ANF provides. Each variable binding in ANF creates an explicit dependency relationship, making it straightforward to construct a dependency graph where nodes represent variables and edges represent usage relationships. This directed graph can then be systematically traversed to identify nodes with no dependents, indicating unused computations. By removing these unreachable nodes from the graph and their corresponding code from the program, we eliminate dead code without altering program semantics. This approach is particularly effective because ANF's explicit naming of intermediate values makes dependencies immediately visible, enabling more precise identification of truly dead code than would be possible with nested expressions

(The general DCE algorithm structure is outlined in the "Dead Code Elminiation" algorithm in "Core Algorithms" in Appendix \ref{alg:dce}).

\subsubsection{Constant Folding}
Constant folding represents the second optimisation technique implemented alongside dead code elimination. This transformation becomes particularly elegant within the ANF representation, where all expressions are explicitly bound to variables. The structure of ANF guarantees that every node in the program representation is either a function application or an atomic value.
By leveraging this, the compiler can systematically traverse the ANF structure to identify expressions whose operands are all compile-time constants. These expressions can then be evaluated during compilation rather than at runtime, replacing the original expression with its computed result. For example, expressions like \texttt{(+ 3 4)} can be reduced to the constant \texttt{7}.\newline

This optimisation produces benefits such as reducing the instruction count in the generated code, eliminating unnecessary computations. This optimisation was chosen as it can be implemented with basic functionality initially and extended to handle increasingly complex constant expressions as needed.
Unlike dead code elimination, which removes unused code paths, constant folding transforms the code while preserving its semantic meaning. The combination of these two optimisations working on the ANF representation demonstrates the power of well-chosen intermediate representations in enabling effective compiler transformations
(The general Constant Folding algorithm structure is outlined in the "Constant Folding" algorithm in "Core Algorithms" in Appendix \ref{alg:const-fold}).

\subsubsection{Three Address Code}
Three Address Code (TAC) is a lower-level intermediate representation where each instruction typically involves at most three addresses (two operands and one result), closely mirroring the structure of assembly language instructions. In this representation, complex expressions are decomposed into a sequence of simpler operations, with each operation producing a result that is stored in a temporary variable. This linear format eliminates nested expressions entirely, making the control flow and data dependencies explicit.\newline

TAC was selected as the second intermediate representation in the compilation pipeline for several strategic reasons. First, it provides a natural progression from ANF, further flattening the program structure while preserving the essential computational logic. Second, TAC's direct correspondence to QBE IR facilitates straightforward code generation, as each TAC instruction can typically be translated into one or a small number of QBE instructions. Finally, TAC enables a range of powerful optimisations that operate effectively on this representation.\newline

The optimisations implemented at the TAC level include scope hoisting, which elevates variable definitions to their optimal scope level, reducing unnecessary allocations and improving memory access patterns. Additionally, inline definitions allow for replacing variable references with their values directly where beneficial, reducing indirection and potentially enabling further optimisations. These transformations are particularly well-suited to the TAC representation due to its explicit control flow, which makes dependency analysis more straightforward and precise. Unlike ANF optimisations which focus on expression simplification, TAC optimisations leverage the linear instruction sequence to identify broader structural improvements that align well with QBE IR's execution model.

\subsubsection{QBE Code Generation}
After experimentation with direct assembly code generation, QBE Intermediate Language \cite{qbe_il} was selected as the final compilation target for this project. QBE (Quick Backend) provides an ideal balance between low-level control and cross-platform compatibility, significantly accelerating the development process while maintaining fine-grained control over the generated code.\newline

QBE offers several strategic advantages for this compiler implementation. First, its streamlined instruction set closely resembles assembly while abstracting away architecture-specific details, enabling the generation of efficient code for multiple target platforms without maintaining separate codebases. Second, QBE's simplified type system which comprises the essential types w (word), l (long), s (single), and d (double) maps to Scheme's runtime representation needs without introducing unnecessary complexity.\newline

The translation from TAC to QBE is direct, as both share a similar three-address code structure. Most TAC operations map directly to QBE instructions, with minimal adaptation required. Memory operations map cleanly to QBE's load and store instructions, which handle different data widths explicitly.
Control flow translation is equally straightforward, with QBE's block-based structure accommodating the program's flow graph representation. Conditional branches in TAC translate to QBE's jnz instruction, while function calls align with QBE's call mechanism. This direct correspondence significantly simplified the code generation phase of the compiler, allowing development efforts to focus on optimisation rather than architecture-specific details.\newline

By leveraging QBE, the compiler achieves a balance of performance, portability, and development efficiency that would be difficult to accomplish with direct assembly generation. The resulting compilation pipeline delivers robust, optimised code while maintaining the flexibility to target multiple architectures with minimal additional effort.

\subsubsection{Runtime System and Garbage Collection Methodology}
\label{sec:methodology_runtime_gc}

Scheme's design, as specified by the R7RS specification, with its dynamic data structures, first-class closures, and functional patterns encouraging immutable data, inherently necessitates automatic memory management \cite{JonesHoskingMoss2011}. Implementing the required dynamic memory allocation and reclamation manually in the target assembly language would be prohibitively complex for this project, a challenge addressed in many high-level languages like Go \cite{GoWebsite}, Java \cite{oracle2024java}, and Python \cite{PythonWebsite} through integrated runtimes with garbage collection. Consequently, a carefully chosen Garbage Collector (GC) is a critical component of the Jaws runtime. Foundational GC concepts and algorithms are surveyed in works such as Wilson \cite{Wilson1992} and comprehensively detailed in Jones, Hosking, and Moss \cite{JonesHoskingMoss2011}.\newline

Several GC strategies were evaluated. Reference counting \cite{Collins1960, JonesHoskingMoss2011}, while offering the potential for immediate reclamation, proves unsuitable for Scheme due to its inability to manage cyclic data structures which can lead to memory leaks and the significant overhead of frequent count updates. Tracing collectors, which identify live objects by traversing from a root set, overcome the cycle problem. The classic Mark-Sweep algorithm \cite{mccarthy1960recursive, JonesHoskingMoss2011}, for instance, marks reachable objects and then sweeps the heap to reclaim unmarked ones. While effective against cycles, Mark-Sweep can lead to heap fragmentation and typically involves "stop-the-world" pauses.\newline

To address fragmentation, Mark-Compact collectors \cite{JonesHoskingMoss2011} extend Mark-Sweep by adding a phase that relocates live objects into a contiguous block, improving memory locality at the cost of increased complexity and similar pause times. Copying collectors \cite{FenichelYochelson1969, Cheney1970, JonesHoskingMoss2011}, which move live objects between distinct heap semi-spaces, inherently provide compaction and can offer fast allocation. However, they typically require doubling the total heap memory, a significant consideration.\newline

Considering these trade-offs, a \textbf{Mark-Sweep-Compact} tracing garbage collector was selected for the Jaws runtime. This methodology effectively handles Scheme's cyclic data structures, a non-negotiable requirement. The compaction phase is crucial for mitigating memory fragmentation over time, leading to more efficient memory use and potentially better cache performance. While more complex than a simple Mark-Sweep, and still involving collection pauses, this approach was deemed a good balance of effectiveness, implementation feasibility within the project's scope, and memory overhead, particularly when compared to the higher memory demands of copying collectors.
\section{Implementation}
This section details the practical development of Jaws. It outlines the actual development based on the above methodology. The implementation has the same three part structure as the methodology. A RSR7 specifciation compliant Scheme interpreter, a compiler that generates efficient machine code via the QBE intermediate representation, and a web-based interface for practical usability.
Full implementation files for any part of the Implementation can be found in appendix \ref{app:jaws-source-files}.


\subsection{Architecture Overview}
The implementation follows a functional pipeline architecture, where each component operates independently and transforms its input into a representation suitable for the next stage. This pipeline design offers several key advantages:

\begin{itemize}[noitemsep]
\item \textbf{Component Independence}: Each stage operates as a pure function, transforming data without shared state. This isolation simplifies testing and modification of individual components.
\item \textbf{Extensibility}: New transformations or optimisations can be inserted at any pipeline stage without affecting other components.
\item \textbf{Functional Purity}: The stateless design aligns naturally with Scheme's functional paradigm, particularly beneficial for implementing features like the reader and evaluator procedures.
\end{itemize}
The implementation is structured into three main subsystems:
\subsubsection{Core Language Implementation}
The interpreter's foundation consists of built-in procedures implemented in the host language, organised into specialised namespaces, which provide essential functions such as equality and type predicates, input/output operations and more. For the full implementation of these built-in procedures, see Table~\ref{tab:jaws-interpreter-frontend-files} in Appendix~\ref{app:jaws-source-files}.
These core procedures provide the essential functionality required by the R7RS specification, serving as building blocks for more complex features.

\subsubsection{Standard Library}
Following R7RS conventions, much of the language's functionality is implemented in Scheme itself, for full details see appendix \ref{tab:stdlib}.  This approach:
\begin{itemize}[noitemsep]
\item Demonstrates the language's expressiveness
\item Provides a clear separation between core and derived features
\item Enables easy modification and extension of standard procedures
\end{itemize}
The import system allows these library procedures to be dynamically loaded, supporting modular program construction and code reuse.
\subsubsection{Runtime System}
The compiler's output interfaces with a runtime system implemented in C, for full details see appendix \ref{tab:jaws-runtime-files}. This runtime:
\begin{itemize}[noitemsep]
\item Provides low-level memory management and garbage collection
\item Implements primitive operations required by compiled code
\item Bridges between compiled Scheme code and the operating system
\end{itemize}
The runtime is compiled as a dynamic library and linked with the generated machine code, providing essential services while maintaining flexibility in implementation and optimisation.
This architectural approach, combining functional purity with clear component boundaries, proved particularly effective for implementing Scheme's dynamic features while maintaining code quality and testability. The following sections detail the implementation of each major component, beginning with the lexical analyser.

\subsection{Lexing and Parsing}
The initial stage of processing Scheme code involves transforming the raw character stream into a structured representation suitable for interpretation or compilation. This frontend process consists of two main phases, lexical analysis (lexing) and syntactic analysis (parsing).\newline

Lexical analysis converts the input text into a sequence of tokens. Following the methodology, a regular expression-based approach was implemented using the C++ standard \texttt{<regex>} library. Token patterns, derived from the R7RS specification, are prioritised to correctly handle overlapping definitions; for example, complex numbers before simple integers. A \texttt{ScanState} struct encapsulates the lexer's state (position, line/column numbers), which is passed functionally through scanning routines. This stateless design is crucial for reusability. Keywords are handled efficiently via a two-stage process: first identifying potential identifiers with a general regex, then checking against a keyword map. Whitespace and comments are discarded, and any unrecognised characters result in a \texttt{ParseError}. The output is a flat list of tokens representing the essential syntactic elements of the code.

The parser then consumes this token stream to construct an Abstract Syntax Tree (AST) reflecting the program's nested S-expression structure. A \texttt{ParserState} struct tracks progress through the token list. The core parsing logic resides in a central dispatch function \texttt{parseExpression}, which determines the appropriate parsing routine based on the current token. For example a \texttt{LEFT\_PAREN} followed by a let initiates a let expression parsing function. \texttt{'} initiates quote parsing, tokens not preceded by a paren are treated as atoms. Parsing functions recursively consume tokens and build corresponding AST nodes, represented using \texttt{std::shared\_ptr} to \texttt{Expression} variants.\newline

Crucially, the parser identifies expressions in tail position (the final evaluation step within a procedure body or conditional branch) and marks them by wrapping the corresponding node in a \texttt{TailExpression}. This explicit marking, performed before macro expansion, is essential for enabling guaranteed tail call optimisation (TCO) in later stages. Basic error recovery is implemented using a "panic mode" strategy \cite{nystrom2021crafting}, attempting to synchronise to the next recognisable top-level form after an error to report multiple issues in one pass. The final output of this stage is a complete AST, ready for macro expansion. 

This functional, stateless design of the lexer and parser components proved particularly advantageous for implementing dynamic Scheme features. Specifically, the \texttt{read} procedure, which parses Scheme data at runtime from input ports, could readily invoke the frontend components on demand without conflicting with the main execution state. Similarly, this clean separation supports the implementation of \texttt{eval}, which requires the runtime interpretation of data structures often produced by \texttt{read}.


\subsection{Macro Expansion}

Scheme's R7RS standard defines hygienic macros primarily through the \linebreak\texttt{syntax-rules} mechanism. A macro defined this way consists of several key components: a set of \emph{literals}, which are identifiers that must match exactly, and a series of \emph{rules}. Each rule is a pair containing a \emph{pattern} and a \emph{template}. The pattern is an S-expression structure containing any literals and \emph{pattern variables}. The macro system attempts to match the input code against these patterns. The ellipsis symbol is used within patterns to indicate that a preceding sub-pattern can match zero or more times. If a pattern matches, the corresponding \emph{template} dictates how the output code should be constructed, substituting the captured pattern variables into the template structure. This transformation process occurs in a dedicated phase between parsing and evaluation/compilation.\newline

Jaws implements this R7RS-compliant \texttt{syntax-rules} system by processing the AST after parsing. The core architecture involves several key components working together. Firstly, to facilitate the pattern matching and template instantiation required by \linebreak\texttt{syntax-rules}, \textbf{a novel design introduced in this project} involves converting the standard \texttt{Expression} AST nodes into a simpler internal representation, termed \linebreak\texttt{MacroExpression}. This \texttt{MacroExpression} consists of nested lists (MacroList) and atomic values (MacroAtom), closely mirroring Scheme's S-expression structure. This simplification is key because \texttt{MacroExpression} has fewer distinct node types and carries less metadata compared to the full \texttt{Expression} AST, making it more directly amenable to the structural pattern matching algorithms required by \linebreak\texttt{syntax-rules}, thus simplifying the recursive traversal and comparisons inherent in macro processing. Secondly, macros defined via \texttt{define-syntax} are stored within a dedicated \texttt{MacroEnvironment} data structure. This environment is distinct from the runtime \texttt{Environment} and maps macro keywords to their transformer definitions.\newline

The expansion engine then iteratively traverses the input program as a \linebreak\texttt{MacroExpression}. When a potential macro invocation is found, the engine attempts to match the input form against the macro's defined patterns. Successful pattern matching captures parts in a dedicated match environment henceforth \texttt{MatchEnv}, associating pattern variables with the \texttt{MacroExpression} nodes they matched. The \texttt{MatchEnv} accommodates ellipsis by mapping a pattern variable to a list of \texttt{MacroExpression} nodes if it matches multiple input forms. For an example see the "Letrec Macro Expansion" item in the "Jaws Website Visuals" Appendix \ref{app:compiler-ex-visuals}.\newline

Following a successful match, the corresponding template is instantiated using the captured bindings from the \texttt{MatchEnv}. During this instantiation, identifiers originating from the template itself are assigned a \texttt{SyntaxContext} based on the macro's definition scope, while identifiers substituted from the \texttt{MatchEnv} retain their original \texttt{SyntaxContext} from the macro call site. The macro system ensures hygiene is maintained through these \texttt{SyntaxContext} objects. These contexts track the scope of each identifier; identifiers introduced within a macro definition receive fresh marks associated with that macro's definition context, while identifiers captured from the macro call site retain their original context, thus preventing unintended variable capture.\newline

The expansion process is iterative; the output of one expansion is re-scanned for further macro calls until no more \texttt{syntax-rules} macros can be applied. A recursion depth limit safeguards against infinite expansion. Finally, the fully expanded \texttt{MacroExpression} tree is converted back into the standard \texttt{Expression} AST format, which is then ready for the subsequent interpretation or compilation stages. This architecture allows Jaws to handle the complexities of hygienic macro expansion while maintaining a clear separation between macro processing and core language evaluation.Â 

\subsection{Interpreter: Execution Engine}Â 
The interpreter forms the the core of the Jaws system, bringing the parsed and macro-expanded Scheme code to life. Operating after the macro expansion pass, it traverses the final AST, applying Scheme's evaluation rules defined by R7RS to compute the program's result using a tree-walk approach. The core dispatch function, \texttt{interpret}, utilises \texttt{std::visit} on the \texttt{Expression} variant AST nodes produced by the frontend and macro expander.

\subsubsection{Evaluation: Standard Calls vs. Special Forms}

Scheme evaluation distinguishes between standard procedure calls and essential special forms. For standard calls Scheme semantics dictates applicative-order evaluation. The operator expression is evaluated first, followed by the operand expressions. If the operator evaluates to a procedure it is invoked. This is handled via a dedicated function \texttt{executeProcedure} with the list of evaluated operand values.

Special forms such as, \texttt{if}, \texttt{lambda}, \texttt{define}, \texttt{set!}, \texttt{quote}, \texttt{quasiquote}, etc, deviate from these standard evaluation rules. They typically control argument evaluation or manipulation of the environment. Jaws handles these with dedicated logic branched from the main \texttt{interpret} dispatch function. The "Scheme Special Forms and Evaluation Rules." list in the "Interpreter Implementation Details" Appendix Table~\ref{tab:special-forms-appendix}summarises these forms and their specific evaluation rules, and program examples can be found in Appendix \ref{app:compiler-ex-visuals}. This dispatch mechanism ensures correct semantic interpretation of the expanded AST.

\subsubsection{Runtime Values}

All runtime values manipulated by the interpreter are uniformly represented by the \texttt{SchemeValue} class. This approach ensures type safety while handling Scheme's dynamic typing, using \texttt{std::variant} internally to hold one of the possible underlying C++ types that correspond to the various Scheme data types defined by R7RS. The specific mapping between Scheme types and their internal representations is detailed in the "Runtime Value Representations" in the "Interpreter Implementation Details" Appendix \ref{tab:runtime-values-appendix}). The \texttt{SchemeValue} class provides predicate methods such as \texttt{isNumber()}, \texttt{isList()} and accessor methods are such as, \texttt{asNumber()}, \linebreak\texttt{asList()}. These are implemented for safe type checking and value retrieval at runtime. Understanding this value representation is key before looking at how environments and closures are handled.

\subsubsection{Environment, Closures, and Control Flow}

The interpreter manages state and control flow through its environment structure and specific mechanisms for tail calls and continuations, operating on the \texttt{SchemeValue} objects described above.

\paragraph{Environment and Hygiene:}Lexical scoping is implemented via a hierarchical \linebreak\texttt{Environment} class. Each instance holds local bindings in a \texttt{std::unordered\_map} mapping \texttt{HygienicSyntax} keys to SchemeValues. Each environmet holds a \linebreak\texttt{std::shared\_ptr} to its parent Environment. Crucially, environment operations which include \texttt{get}, \texttt{set}, and \texttt{define} respect macro hygiene by comparing both the identifier's name and its associated \texttt{SyntaxContext} marks within the \linebreak\texttt{HygienicSyntax} object provided by the macro-expanded AST. This ensures the interpreter correctly resolves references according to the scope rules preserved by the hygienic macro system.

\paragraph{Closures:}Jaws implements closures, a core feature of functional programming. A closure is formed when a procedure (often defined within another scope or returned by another procedure) captures the lexical environment active at the time of its definition. This captured environment includes bindings for variables defined in its parent and ancestor scopes. For an example program see Closures in Appendix \ref{app:compiler-ex-visuals}. This is handled via a \texttt{UserProcedure} object which is a type represented in the variant in \linebreak\texttt{SchemeValue}. This object captures a pointer to the lexical environment at definition time. When the procedure is later invoked, its body is evaluated within a new environment created by calling \texttt{extend} (a function which creates a new environment scope) on this captured closure environment, granting access to the correct lexical scope.

\paragraph{Tail Call Optimisation (TCO):}TCO is implemented using trampolining. The parser marks tail positions using \texttt{TailExpression} nodes. When the interpreter encounters a procedure call within a \texttt{TailExpression} that is eligible for TCO, instead of recursing directly, the interpreter stores a reference to the procedure and its arguments in its internal state and returns a specific signal. The main procedure execution loop checks for this state at the start of each iteration; if set, it loads the pending call details and \emph{continues the loop} without creating a new C++ stack frame, thus achieving constant stack space. For an example program running in the Jaws website editor see an Appendix item \ref{app:compiler-ex-visuals} and for the general trampolining algorithm see the "Trampolining Algorithm for Tail Call Optimisation" in the "Core Algorithms" Appendix Algorithm~\ref{alg:trampoline})..

\paragraph{Continuations:}First-class continuations are implemented using C++ exceptions for non-local control transfer. The \texttt{call/cc} built-in captures the current \linebreak\texttt{InterpreterState} including the environment chain and pending values into a \texttt{Continuation} object represented as a \texttt{SchemeValue}. If this \texttt{Continuation} object is later invoked, it throws a special \texttt{ContinuationInvocationException} containing the captured state and the invocation argument(s). The main interpretation loop catches this exception, restores the interpreter's state entirely from the exception object, and uses the passed value as the result of the restored context.

\subsubsection{Quoting and Unquoting}
Scheme's ability to treat code as data is handled by quoting mechanisms.\linebreak The \texttt{interpretQuote} prevents evaluation of its operand. The interpreter uses a helper function \texttt{expressionToValue} to recursively convert the unevaluated \linebreak\texttt{Expression} AST node into corresponding runtime \texttt{SchemeValue} data structures. \texttt{interpretQuasiQuote} enables constructing data with evaluated parts, working with another recursive helper function \texttt{processQuasiQuote} to handle nested \linebreak\texttt{unquote} and \texttt{unquote-splicing} directives within the quasiquoted structure.Â 

\subsection{Standard Library and Built-ins} % Combined Standard Lib/Imports
Jaws follows R7RS practice by implementing a significant portion of the standard library in Scheme itself, supported by core primitives implemented in C++.
\paragraph{Built-in Procedures:} The interpreter's initial environment is populated with built-in procedures implemented directly in C++. These are represented as \texttt{SchemeValue}'s containing a pointer to a BuiltinProcedure. This class wraps a C++ callable \linebreak\texttt{std::function} which directly executes the primitive operation, receiving the current \texttt{InterpreterState} and evaluated arguments. For further details on builtins see Appendix \ref{tab:jaws-interpreter-frontend-files}.
\paragraph{Scheme Standard Library and Imports:} Higher-level standard library can be found in Appendix \ref{tab:stdlib}. These libraries define many standard functions and syntax in terms of simpler primitives or other library procedures. The import system handles loading these libraries. When an \texttt{import} declaration is encountered, it resolves the library path, parses the expression, checks dependencies and caches the library data in a \texttt{LibraryRegistry}, and populates the interpreter's runtime \texttt{Environment} and the expander's \texttt{MacroEnvironment} with the exported bindings and syntax definitions. This allows modularity and demonstrates the interpreter's capability to execute substantial Scheme code.

\subsection{Compiler}Â 
Beyond interpretation, Jaws includes a compiler designed to translate Scheme code into efficient native machine code. This compiler follows a multi-stage pipeline, transforming the input Scheme program through several IRs before generating code suitable for execution. The pipeline utilises ANF for initial transformation and high-level optimisations, followed by TAC for lower-level representation, ultimately targeting the QBE Intermediate Language for backend code generation. The compiled code interfaces with a custom C runtime system responsible for memory management, primitive operations, and data representation.


\subsubsection{ANF Transformation}

The compilation process begins after parsing and macro expansion.\linebreak An \texttt{ANFTransformer} component recursively traverses the input \texttt{Expression} AST, converting from Expression to ANF. This representation decomposes complex expressions, binding intermediate results to temporary variables. A key characteristic of ANF is that all arguments to functions must be atomic (variables or constants). Complex expressions are decomposed into a sequence of \texttt{let}-bindings, where each \texttt{let} introduces a new temporary variable for an intermediate computation. Thus, the overall computation flows through these nested \texttt{let}s, each typically binding a single, simple expression.\newline

The core ANF data structure, \texttt{ANF}, uses a \texttt{std::variant} to represent node type.
This transformation guarantees that arguments within function applications are atomic and all arguments are represented by variables or constants, making the evaluation order explicit. Top-level definitions become root nodes, which hold the ANF expressions. A final \texttt{flattenLets} pass optimises the nested \texttt{Let} structures. The specific rules for converting the AST to ANF are detailed in the "Key AST to ANF Transformation Rules." list in the "IR Transformation Rules" Appendix \ref{tab:anf-transformations-appendix}.

\subsubsection{ANF Optimisations}

Once the code is in ANF, an optimisation phase applies transformations. \textbf{Constant Folding} traverses the ANF tree, maintaining an environment of known constant values. It identifies applications represented by ANF \texttt{App} Nodes, of pure primitive functions whose arguments are all compile-time constants and evaluates them. This results in replacing the application with an \texttt{Atom} node containing the result. Constant conditions represented in ANF by \texttt{If} nodes are also evaluated, potentially eliminating branches. For a visual example see "Dead Code Elimination" in the "Jaws Website Visuals" Appendix \ref{app:compiler-ex-visuals}.\newline

\textbf{Dead Code Elimination (DCE)} removes code that does not impact the program's result. This involves building a dependency graph from the ANF bindings, identifying "live" nodes by tracing dependencies backward from essential roots. The optimiser then filters out any top-level definition nodes that are not identified as live and are not reachable from these essential roots. Special care must be taken for side-effecting operations or the main program result as they may not have out-neighbours however they must not be removed. For a visual example see "Constant Folding" in the "Jaws Website Visuals" Appendix \ref{app:compiler-ex-visuals}.


\subsubsection{Three Address Code (TAC) Conversion}

Following ANF conversion, the IR is lowered to TAC, a linear, machine-like intermediate representation. A dedicated conversion function \texttt{anfToTac}, translates the ANF structure into a sequence of \texttt{ThreeACInstruction} stored within a \linebreak\texttt{ThreeAddressModule}. Each instruction typically involves an operation code and up to three operands a result, along with arg1, arg2; making control flow and data dependencies explicit. This flattening process eliminates nested expressions. Lambda expressions encountered during conversion are collected, and their bodies are translated into separate TAC sequences, delimited by \texttt{FUNC\_BEGIN} and \texttt{FUNC\_END} instructions. The detailed ANF to TAC translation rules are summarised in the "Key ANF to TAC Transformation Rules." list in the "IR Transformation Rules" Appendix \ref{tab:anf-to-tac-transformations-appendix}.

\subsubsection{QBE Code Generation}

The final compiler stage converts the TAC instruction sequence into the QBE Intermediate Language. A \texttt{QBEGenerator} component performs this translation. It generates a QBE data section defining runtime constants such as, \texttt{\$nil\_obj}) and data labels for string literals. It then generates the QBE \texttt{\$main} function from the initial global TAC instructions, followed by QBE function definitions derived from TAC sequences bracketed by \texttt{FUNC\_BEGIN} and \texttt{FUNC\_END}. A core dispatch function maps each TAC \texttt{Operation} to its equivalent QBE instruction(s), according to the rules outlined in the "Key TAC to QBE Transformation Rules." list in the "IR Transformation Rules" Appendix \ref{tab:qbe-transformations-appendix}. The resulting QBE code is written to an output file.

\subsubsection{Runtime System Interaction}
The QBE code generated by the compiler executes in along with a custom C runtime system, for further details see Appendix \ref{tab:jaws-runtime-files}. The runtime manages memory via a garbage-collected heap and provides necessary data structures like environments and closures. Compiled code allocates Scheme objects bloated structs representing pairs, closures, etc on a heap by calling the \texttt{alloc\_object} function. This function manages a contiguous memory region the programs \texttt{heap}; if allocation exceeds available space (\texttt{used > heap\_size}), it triggers the garbage collector (\texttt{gc}) and, if still necessary, attempts to increase the heap size via \texttt{realloc} (\texttt{grow\_heap}).\newline

The garbage collector implements a mark-sweep-compact strategy. The \texttt{mark\_roots} function initiates the mark phase by identifying initially reachable objects, including those accessible from the global environment and symbol table as well as potential pointers found via a conservative scan of the C stack between an approximate bottom and the current stack pointer. The recursive \texttt{mark\_object} function then traverses reachable heap objects following \texttt{car} and \texttt{cdr} pointers in pairs as well as environment pointers in closures. The function sets a mark bit (\texttt{GC\_MARK\_BIT}) within the \texttt{type} field of each live \texttt{SchemeObject}. Following marking, the \texttt{sweep\_compact} function performs compaction in three passes: first, it calculates the new addresses for all marked objects which are considered "live" and stores these in a temporary forwarding map. It then iterates through the live objects again, updating any internal heap pointers to point to the new forwarding addresses.\newline

Finally, it moves the live objects contiguously to the beginning of the heap using \texttt{memmove} and updates the heap usage counter. After compaction, \texttt{try\_shrink\_heap} may attempt to reduce the heap size via \texttt{realloc} if utilisation is below a threshold. Closures required by compiled lambda expressions are created via the runtime's \texttt{make\_closure} function, which allocates a \texttt{SchemeObject} of \texttt{TYPE\_FUNCTION} and stores both the pointer to the compiled native code via a \texttt{void*} and the pointer to the captured runtime lexical environment represented by \texttt{SchemeEnvironment*} within the object's \texttt{value.function} field. Compiled code interacts with these runtime structures and calls other C runtime functions for essential operations like symbol interning or environment manipulation.

\subsection{Web Interface}
The Jaws web interface provides an interactive online environment where users can write, execute, and learn the Jaws language. This single-page application (SPA) is constructed using modern web technologies to ensure a responsive and intuitive user experience.\newline

The frontend's technology stack is built primarily with React for its component-based UI development, and TypeScript for enhanced code quality through static typing across \texttt{.ts} and \texttt{.tsx} files. Styling is managed by the utility-first CSS framework Tailwind CSS. The frontend project is built and served using Vite a fast development server and build tool.\newline

Architecturally, the C++ Jaws interpreter is compiled to WebAssembly (WASM), enabling it to run directly within the browser for near-native execution performance. A special WebAssembly wrapper class \texttt{JawsWrapper.cpp}, wraps the C++ interpreter and exposes a JavaScript-callable interface, which is subsequently compiled to WASM. A custom React hook, \texttt{useJawsInterpreter.ts}, serves as the JavaScript bridge; it manages the loading of the WASM module and facilitates communication between React components and the Jaws interpreter, handling code submission, result retrieval, and error reporting.\newline 

The web interface offers several key features and views to support users. A Welcome Page acts as the initial landing point, introducing Jaws and guiding users to other sections. The central feature is the Interactive Editor \& Playground, which incorporates an embedded code editor where users can write and execute Jaws code. The output, including print statements and errors, is displayed in an integrated terminal-like component. For educational purposes, a Learning Platform provides structured content, including chapters defined in \texttt{chapters.ts} and illustrative code examples from \texttt{examples.ts}. An advanced Compiler Explorer allows users to input Jaws code and observe various stages of the experimental compilation process, such as the A-Normal Form (ANF), Three-Address Code (3AC), serving as a valuable tool for understanding compiler internals. Finally, the interface is designed for Responsive Design, employing Tailwind CSS and modern React practices, with specific considerations for mobile devices.\newline

Overall, the web interface is designed as a comprehensive tool for both learning the Jaws language and experimenting with its capabilities, directly harnessing the power of the C++ interpreter via WebAssembly. For visual examples see Appendix \ref{app:compiler-ex-visuals}.\newline

The Jaws Interpreter Website and its source code are publicly available at: \url{https://jamie-wales.github.io/Jaws}



\section{Evaluation} % Or \subsection{Evaluation} if it's within Implementation

Evaluating the correctness and capability of a programming language implementation, such as the Jaws interpreter and compiler, presents significant challenges. Unlike typical software testing, verifying adherence to a complex language specification like R7RS across countless possible program inputs is practically infeasible. Furthermore, assessing the "quality" of an implementation involves not just correctness but also factors like performance (for the compiler) and the ability to handle realistic, non-trivial programs. Therefore, the evaluation of Jaws employed a two-pronged approach aimed at providing confidence in both its semantic correctness relative to the R7RS standard and its practical utility.\newline

The first approach focused on demonstrating the capability of Jaws by executing non-trivial Scheme programs. The ability to successfully run larger, more complex code provides a qualitative assessment of the implementation's completeness and robustness. If Jaws can handle programs that utilise a significant range of language features, interact with the standard library extensively, or implement complex algorithms (potentially including parts of the Jaws standard library itself, which is written in Scheme), it lends credence to the claim that the implementation is functional and practical beyond simple test cases. This approach helps ascertain if the core language, runtime system, and library support work together coherently for meaningful computation.\newline

The second approach involved comparative testing against an established, R7RS-compliant Scheme implementation, specifically Chez Scheme \cite{chezscheme}. This method acts as a black-box validation technique. A suite of Scheme test programs, covering various language features and standard library functions, was executed on both Jaws (primarily the interpreter for semantic comparison) and Chez Scheme. The core validation criterion was that for a given test program, Jaws should produce the exact same output, or exhibit the same behaviour, as Chez Scheme. Obtaining matching results across a diverse test suite provides strong evidence that Jaws correctly implements the R7RS semantics for the tested functionalities, effectively using Chez Scheme for a benchmark for expected behaviour. While this doesn't formally prove correctness, consistent agreement with a mature implementation significantly increases confidence in Jaws's adherence to the standard.

\subsection{Non-Trivial Program Evaluation}
\label{sec:nontrivial-eval}

To assess the practical capability and robustness of Jaws beyond basic conformance tests, several non-trivial programs were developed and executed using the interpreter. The successful execution of these examples, which utilise advanced features like concurrency, networking, and FFI, provides qualitative evidence that Jaws can support complex, real-world use cases. The Scheme code, along with relevant outputs or visual examples for the producer-consumer, Snake game, and meta-circular evaluator discussed herein, are detailed in Appendix~\ref{tab:non-trivial}.\newline

The first example implements the classic producer-consumer problem, testing Jaws' threading and synchronisation primitives. This program was executed successfully, demonstrating correct thread interaction, mutual exclusion via mutexes, and inter-thread signaling using condition variables.\newline

The second set of examples validates the basic TCP/IP networking capabilities provided by the socket primitives. A simple client and server were implemented. Running these together resulted in successful communication, with the client connecting, sending a message, receiving a reply, and disconnecting, confirmed by the client output shown. This confirms the functionality of the core socket API for establishing and managing basic network connections.\newline

The third, and perhaps most complex, example demonstrates the Foreign Function Interface (FFI). A simple Snake game, with graphics and core logic implemented in C using the Raylib library, was controlled entirely from Scheme. Jaws' FFI primitives were used to load the compiled C library and bind C functions to Scheme procedures based on their type signatures. The main game loop resides in Scheme, making calls to the C functions for initialisation, input handling, game state updates, and rendering. The successful execution and interactive playability of the game validates the FFI mechanism's ability to load external libraries, register functions with diverse type signatures (int, void, string), and correctly marshal data between the Scheme environment and native C code.\newline

Finally, drawing inspiration from the meta-circular evaluator presented in "Structure and Interpretation of Computer Programs" (SICP) \cite{structureandinterp}, a basic interpreter for a subset of Scheme focused on mathematical expressions was implemented \textbf{in Scheme itself}. This evaluator handles numbers, variable definition (\texttt{define}), variable lookup using an association list for the environment, and application of primitive arithmetic procedures (\texttt{+}, \texttt{-}, \texttt{*}, \texttt{/}). Running this math evaluator within the main Jaws interpreter tests the core evaluation logic, environment handling, function application and primitive procedure integration of Jaws itself. Successfully executing this meta-evaluator demonstrates a significant level of correctness and completeness in the underlying Jaws implementation, as the interpreter must correctly interpret the code that defines its own evaluation rules. These files and visual examples of them working can be found in Appendix \ref{tab:non-trivial}. \newline

\subsection{Comparative Testing against Chez Scheme}

\label{sec:chez-testing}

As outlined in the overall evaluation strategy, a key method for validating the semantic correctness of the Jaws interpreter was through comparative testing against a mature, R7RS-compliant Scheme implementation. For this purpose, Chez Scheme \cite{chezscheme} was chosen as the reference.\newline

A dedicated test suite, written in R7RS-compliant Scheme, was developed to systematically exercise a range of fundamental language features. This suite, presented in Listing~\ref{tab:non-trivial}, encompasses several categories of tests:
\begin{itemize}[noitemsep]
    \item \textbf{Basic Arithmetic:} Simple addition and subtraction to verify numerical primitives.
    \item \textbf{Recursive Functions:} Implementation of factorial and Fibonacci sequences to test recursive evaluation and base cases.
    \item \textbf{List Processing:} Procedures for summing elements of a list and a custom implementation of \texttt{my-map} (a simplified version of the standard \texttt{map}) to test operations on list structures and higher-order function application.
    \item \textbf{Local Bindings and Numeric Precision:} A function to calculate quadratic roots using \texttt{let*} for local bindings, also testing floating-point arithmetic.
    \item \textbf{Macros:} A simple \texttt{when} macro defined using \texttt{syntax-rules} to test basic hygienic macro expansion and evaluation.
\end{itemize}
Each test case within the script is designed to compare an \texttt{expected} value against the \texttt{actual} result produced by the Jaws interpreter, explicitly printing whether each test passes.\newline

The methodology involved executing this identical test script on both the Jaws interpreter and Chez Scheme. The primary validation criterion was that the Jaws interpreter must produce a byte-for-byte identical output to that of Chez Scheme for the entire test suite. This includes the printed test names, expected values, actual values, and pass/fail status.\newline

The Jaws interpreter successfully processed the entire test suite. The output generated by Jaws, exactly matched the output produced when the same script was run in Chez Scheme. This coverage across all test cases, which includes basic operations, recursion, list manipulation, higher-order functions, local bindings, and macro expansion, provides strong evidence for the R7RS conformance of the Jaws interpreter for these core features.\newline

While this does not constitute an exhaustive proof of complete R7RS compliance the successful outcome of this comparative testing provides a confidence in the correctness and reliability of the Jaws interpreter's implementation of the tested Scheme semantics. The identical outputs affirm that, for this diverse set of functionalities, Jaws behaves as expected according to a well-established R7RS standard.
\section{Conclusion and Future Work}

\subsection{Conclusion}

This dissertation details the development of "Jaws," a Scheme implementation comprising an R7RS-compliant interpreter and a native code compiler. The compiler targets the x86-64 architecture via the QBE intermediate language. A primary objective was to demystify functional language compiler construction by establishing a comprehensive development pathway from theoretical principles to machine code execution. This objective was achieved, providing a clear roadmap for such projects. The selected methodologies including regular expression-based lexing, recursive descent parsing, hygienic macro expansion, trampolining for tail call optimisation (TCO), and a multi-stage compilation pipeline: AST $\rightarrow$ ANF $\rightarrow$ TAC $\rightarrow$ QBE were effective for addressing the challenges of implementing Scheme.\\

The Jaws interpreter implements core R7RS features such as lexical scoping, first-class procedures, closures, hygienic macros, and guaranteed TCO. Its correctness for these features was validated through the execution of non-trivial programs and comparative testing against Chez Scheme. While the interpreter offers a robust R7RS core, complete standard library coverage is a defined area for future extension. Given the availability of the core language functionalities within Jaws, such an extension could potentially be expedited by forking and integrating a suitable open-source R7RS standard library. \\

The compiler translates Scheme to QBE, currently supporting variables, closures, functions, basic arithmetic, and printing. This functionality interfaces with a C runtime system that includes a Mark-Sweep-Compact garbage collector. This work establishes the foundational pipeline for native code generation from Scheme. Attaining full R7RS compliance for the compiler represents a direct extension of this established structure.\\

In summary, the Jaws project delivered a functional Scheme interpreter and the core of a native code compiler, successfully demonstrating the process of translating a high-level functional language to machine code and thereby meeting its primary aim of clarifying this complex domain.
\subsection{Future Work}

The current Jaws implementation provides a solid foundation for several well-defined future enhancements:

\begin{itemize}
    \item \textbf{Full R7RS Interpreter Compliance:} Complete the implementation of the R7RS standard library. Many of these functions can be written in Scheme itself, leveraging the existing core interpreter.
    \item \textbf{Comprehensive R7RS Compiler Support:} Extend the compiler's capabilities to cover the full R7RS language specification. This involves systematically adding translation logic for all remaining data types, special forms, and procedures within the existing ANF/TAC/QBE pipeline.
    \item \textbf{Advanced Compiler Optimisations:} Integrate additional optimisation passes to improve the performance of compiled code. This includes techniques such as common subexpression elimination, function inlining, more extensive data flow analysis, and peephole optimisations on the generated QBE or subsequent assembly.
    \item \textbf{Alternative Execution Strategies:} Develop a bytecode compiler and corresponding Virtual Machine (VM) for Jaws. This could serve as an alternative to the current tree-walk interpreter, potentially offering improved performance and a platform for future Just-In-Time (JIT) compilation research.
    \item \textbf{Runtime System and Tooling Enhancements:} Investigate more sophisticated garbage collection algorithms to reduce pause times and improve memory management efficiency. Develop enhanced debugging and performance profiling tools for both interpreted and compiled Jaws code.
\end{itemize}

These future directions would significantly mature the Jaws system, increasing its R7RS compliance, performance, and utility as both a Scheme implementation and a platform for compiler research.
\newpage

\section{Glossary}
\label{Glossary}
\setlist[itemize]{noitemsep} % Ensure compact list for the glossary
\begin{itemize}
    \item \textbf{Abstract Syntax Tree (AST)}: A tree representation of the syntactic structure of source code, created by the parser and used in subsequent stages of compilation or interpretation.
    \item \textbf{A-Normal Form (ANF)}: An intermediate representation in compilers where every non-trivial expression is bound to a variable, making the order of evaluation explicit and simplifying optimisations.
    \item \textbf{Applicative-Order Evaluation}: An evaluation strategy where all arguments to a function are evaluated before the function itself is applied.
    \item \textbf{Bytecode}: A low-level platform-independent code that is executed by a virtual machine.
    \item \textbf{Chez Scheme}: A high-performance, R7RS-compliant implementation of the Scheme programming language, used in this project as a reference for comparative testing.
    \item \textbf{Closure}: A first-class function that captures (encloses) the lexical environment in which it was defined, allowing it to access variables from that environment even when called outside of it.
    \item \textbf{Compiler}: A program that translates source code written in one programming language (the source language) into another language (the target language), often machine code or an intermediate representation.
    \item \textbf{Constant Folding}: An optimisation technique where expressions whose values are known at compile time are evaluated during compilation, replacing the expression with its result.
    \item \textbf{Continuation}: A data structure representing the "rest of the computation" at a given point in a program's execution, enabling advanced control flow mechanisms like non-local exits.
    \item \textbf{Dead Code Elimination (DCE)}: A compiler optimisation that removes code that has no effect on the program's output.
    \item \textbf{Deterministic Finite Automaton (DFA)}: A finite-state machine that accepts or rejects strings of symbols, commonly used as the theoretical basis for lexical analysers.
    \item \textbf{Environment}: In programming language implementation, a data structure (often a map or association list) that binds identifiers (names) to their corresponding values or storage locations within a given scope.
    \item \textbf{eval}: A procedure in Lisp-like languages (including Scheme) that takes a representation of code as data (typically an S-expression) and executes it.
    \item \textbf{First-Class Functions/Procedures}: A property of programming languages where functions are treated as values that can be assigned to variables, passed as arguments to other functions, and returned as results from other functions.
    \item \textbf{Foreign Function Interface (FFI)}: A mechanism that allows code written in one programming language to call routines or make use of services written in another.
    \item \textbf{Functional Programming}: A programming paradigm that treats computation as the evaluation of mathematical functions and avoids changing-state and mutable data.
    \item \textbf{Garbage Collection (GC)}: A form of automatic memory management where the runtime system reclaims memory occupied by objects that are no longer in use by the program.
    \item \textbf{Homoiconicity}: A property of programming languages in which the primary representation of programs is also a data structure in the language itself. In Scheme and Lisp, code is represented as S-expressions, which are lists.
    \item \textbf{Hygienic Macros}: A macro system that prevents accidental capture of identifiers. Variables in the macro's expansion are distinct from variables in the context where the macro is called, unless explicitly intended.
    \item \textbf{Immutable Data Structures}: Data structures whose state cannot be modified after they are created.
    \item \textbf{Interpreter}: A program that directly executes program instructions, statement by statement or expression by expression, without prior compilation into machine code.
    \item \textbf{Intermediate Representation (IR)}: A data structure or code form used internally by a compiler or virtual machine to represent source code during various stages of processing. ANF and TAC are examples used in Jaws.
    \item \textbf{Jaws}: The name of the Scheme R7RS interpreter and compiler implementation developed in this dissertation.
    \item \textbf{Lambda Calculus ($\lambda$-calculus)}: A formal system in mathematical logic for expressing computation based on function abstraction and application using variable binding and substitution. It is a foundational theory for functional programming.
    \item \textbf{Lexer (Lexical Analyser or Scanner)}: The first phase of a compiler or interpreter that converts a sequence of characters from the source program into a sequence of tokens.
    \item \textbf{Lexical Scoping}: A convention for determining the scope of identifiers (names) based on their position in the source code. An identifier refers to its closest enclosing definition.
    \item \textbf{Lisp (List Processing)}: A family of programming languages with a long history, characterissd by its fully parenthesised prefix notation (S-expressions) and its early adoption of functional programming concepts. Scheme is a dialect of Lisp.
    \item \textbf{Macro Expansion}: The process by which a macro invocation is replaced with its corresponding template, transforming the abstract syntax tree before evaluation or further compilation.
    \item \textbf{Mark-Sweep-Compact}: A type of tracing garbage collection algorithm that first marks all reachable objects, then sweeps (reclaims) unmarked objects, and finally compacts the heap by moving live objects together to reduce fragmentation.
    \item \textbf{Metacircular Evaluator}: An interpreter for a language written in the language itself.
    \item \textbf{Pair}: The most fundamental compound data structure in Scheme, consisting of two components (traditionally called \texttt{car} and \texttt{cdr}). Lists are formed from nested pairs.
    \item \textbf{Parser (Syntactic Analyser)}: The phase of a compiler or interpreter that takes a sequence of tokens from the lexer and builds a data structure (typically an Abstract Syntax Tree) representing the grammatical structure of the input according to a formal grammar.
    \item \textbf{Port}: An abstraction in Scheme for input and output devices, such as files or the console.
    \item \textbf{Pure Functions}: Functions that, given the same input, will always return the same output and do not have any observable side effects such as modifying a global variable or performing I/O.
    \item \textbf{QBE Intermediate Language (QBE IL)}: A specific static single assignment (SSA) based intermediate language used as the target for the Jaws compiler, designed to be simple and facilitate compilation to various machine backends.
    \item \textbf{R7RS (Revised$^7$ Report on the Algorithmic Language Scheme)}: The official standard defining the Scheme programming language, which Jaws aims to implement.
    \item \textbf{Recursive Descent Parsing}: A top-down parsing technique where a set of mutually recursive procedures is used to process the input, with each procedure typically implementing one of the grammar's nonterminals.
    \item \textbf{Regular Expressions (Regex)}: A sequence of characters that defines a search pattern, used in Jaws for tokenising source code during lexical analysis.
    \item \textbf{Runtime System}: The set of services and functions that support the execution of a program, including memory management, primitive operations, and interaction with the operating system.
    \item \textbf{S-expressions (Symbolic Expressions)}: The notation used in Lisp and Scheme for representing both code and data as nested lists and atoms.
    \item \textbf{Scheme}: A minimalist dialect of the Lisp programming language, known for its elegance, lexical scoping, first-class procedures, and hygienic macro system.
    \item \textbf{Side Effects}: Actions performed by a function that modify state outside its local scope or interact with the outside world, for example modifying a global variable or performing I/O.
    \item \textbf{Single-Page Application (SPA)}: A web application or website that interacts with the user by dynamically rewriting the current page rather than loading entire new pages from a server.
    \item \textbf{syntax-rules}: A standard R7RS Scheme form for defining hygienic macros using pattern matching.
    \item \textbf{SyntaxContext}: An internal mechanism in the Jaws macro expander to track the lexical context of identifiers, crucial for ensuring macro hygiene by preventing unintended variable capture.
    \item \textbf{Tail Call Optimisation (TCO)}: A compiler or interpreter optimisation where a function call in a tail position (the last operation before returning) is executed without consuming additional stack space, effectively turning recursion into iteration.
    \item \textbf{Three Address Code (TAC)}: An intermediate representation in which each instruction has at most three operands (typically two source operands and one destination).
    \item \textbf{Token}: A string of characters, categorised according to the rules of the lexer, that is treated as a single indivisible unit in the grammar of the programming language for example keyword, identifier, literal or operator.
    \item \textbf{Trampolining}: A technique for implementing tail call optimisation in languages that do not natively support it, by having tail-recursive functions return thunks (closures representing the next call) that are then executed in a controlling loop.
    \item \textbf{Tree-walk Interpreter}: An interpreter that executes a program by directly traversing its Abstract Syntax Tree and performing actions based on the type of each node encountered.
    \item \textbf{Virtual Machine (VM)}: An abstraction of a physical computer, often used to execute programs written in a bytecode intermediate language.
    \item \textbf{WebAssembly (WASM)}: A binary instruction format for a stack-based virtual machine, designed as a portable compilation target for deploying high-performance applications on the web and other environments.
    \item \textbf{x86-64}: A 64-bit version of the x86 instruction set architecture, targeted by the Jaws compiler for native code generation.
\end{itemize}
\bibliography{reportbib}
\appendix % Start the appendix environment

\setlist[itemize]{noitemsep} % Apply compact item spacing for all itemize lists in the appendix

% --- Appendix A: AST to ANF Transformation Rules ---
\section{IR Transformation Rules}

This section details the translation rules between Intermediate Representations.
\subsubsection*{Key A-Normal Form (ANF) Node Types}
\label{tab:anf-node-types-succinct}
This section concisely describes the A-Normal Form (\anf) node variants used in the Jaws compiler. Each \texttt{ir::NodeName \{components\}} represents a distinct ANF expression type.

\begin{itemize}[itemsep=3pt]
    \item \textbf{\irnode{Let \{name, binding, body\}}}: Binds the result of the \texttt{binding} ANF expression to an optional \texttt{Token name}, then evaluates the \texttt{body} ANF expression in this extended scope to make evaluation order explicit.
    \item \textbf{\irnode{App \{name, params, isTailCall\}}}: Represents a call to the function \texttt{Token name} with a list of atomic \texttt{Token params}; \texttt{isTailCall} is a boolean indicating if it's in a tail position.
    \item \textbf{\irnode{If \{cond, then, \_else\}}}: Represents conditional control flow based on the atomic \texttt{Token cond}, evaluating either the \texttt{then} ANF branch or the optional \texttt{\_else} ANF branch.
    \item \textbf{\irnode{Atom \{atom\}}}: Represents an atomic value, either a literal or a variable identifier, held in the \texttt{Token atom}, requiring no further computation.
    \item \textbf{\irnode{Lambda \{params, body\}}}: Defines an anonymous function with a list of \texttt{Token params} and an ANF \texttt{body}, forming a closure when evaluated.
    \item \textbf{\irnode{Quote \{expr\}}}: Represents a literal Abstract Syntax Tree \texttt{Expression expr} from the source, treated as data and not evaluated.
    \item \textbf{\irnode{TDefine \{name, body\}}}: A top-level definition that binds a global \texttt{Token name} to an ANF \texttt{body} (which is a value or a lambda).
\end{itemize}

\subsubsection*{Key Three-Address Code (TAC) Instruction Types}
\label{tab:tac-instruction-types-succinct}
This section describes the Three-Address Code (\tac) instruction set. Operands \texttt{R}, \texttt{A1}, \texttt{A2} generally refer to result, first argument, and second argument respectively, which can be TAC variables, labels, or literal strings/values.

\begin{itemize}[itemsep=3pt]
    \item \textbf{\texttt{COPY R, A1}}: Assigns the value of source \texttt{A1} to result \texttt{R}.
    \item \textbf{\texttt{LABEL L}}: Defines the label \texttt{L} (from \texttt{A1}) as a jump target in the instruction sequence.
    \item \textbf{\texttt{JUMP L}}: Unconditionally transfers control flow to the instruction at label \texttt{L} (from \texttt{A1}).
    \item \textbf{\texttt{CALL R, F, N}}: Calls function \texttt{F} (from \texttt{A1}) with \texttt{N} parameters (count in \texttt{A2}, parameters passed via preceding \texttt{PARAM} instructions), storing the return value in \texttt{R}.
    \item \textbf{\texttt{JUMP\_IF C, L}}: Jumps to label \texttt{L} (from \texttt{A2}) if the boolean condition in TAC variable \texttt{C} (from \texttt{A1}) is true.
    \item \textbf{\texttt{JUMP\_IF\_NOT C, L}}: Jumps to label \texttt{L} (from \texttt{A2}) if the boolean condition in TAC variable \texttt{C} (from \texttt{A1}) is false.
    \item \textbf{\texttt{ALLOC R, TypeStr, AuxData}}: Allocates memory for an object of type \texttt{TypeStr} from \texttt{A1}) with auxiliary data \texttt{AuxData} (e.g., function label, literal content from \texttt{A2}), storing the object's address in \texttt{R}.
    \item \textbf{\texttt{LOAD R, VarName}}: Loads the value of the Scheme variable \texttt{VarName} (from \texttt{A1}) into TAC variable \texttt{R}.
    \item \textbf{\texttt{STORE VarName, Value}}: Stores the \texttt{Value} (from TAC variable or literal in \texttt{A2}) into the Scheme variable \texttt{VarName} (in \texttt{A1}).
    \item \textbf{\texttt{GC}}: Triggers a garbage collection cycle (no explicit operands).
    \item \textbf{\texttt{PARAM Value}}: Passes \texttt{Value} (from \texttt{A1}) as a parameter for the next \texttt{CALL} instruction.
    \item \textbf{\texttt{RETURN Value}}: Returns from the current function, optionally with \texttt{Value} (from \texttt{A1}).
    \item \textbf{\texttt{FUNC\_BEGIN FuncLabel, ParamNamesStr}}: Marks the beginning of a function definition for \texttt{FuncLabel} (from \texttt{A1}) with an optional comma-separated string of \texttt{ParamNamesStr} (from \texttt{A2}).
    \item \textbf{\texttt{FUNC\_END FuncLabel}}: Marks the end of the function definition for \texttt{FuncLabel} (from \texttt{A1}).
\end{itemize}





\label{app:anf-table}
\subsubsection*{Key AST to ANF Transformation Rules.}
\label{tab:anf-transformations-appendix}
\begin{itemize}
    \item \textbf{\texttt{AtomExpression}}: \texttt{ir::Atom} containing the token.
    \item \textbf{\texttt{sExpression} \texttt{(f x y)})}: Nested \texttt{ir::Let}s binding non-atomic args (\texttt{x}, \texttt{y}) to temps, ending in \texttt{ir::App} using function name token and argument tokens/temps.
    \item \textbf{\texttt{IfExpression}}: \texttt{ir::Let} binding condition to temp \texttt{c}, followed by \texttt{ir::If} using \texttt{c} (token) with transformed branches.
    \item \textbf{\texttt{LambdaExpression}}: \texttt{ir::Lambda} node containing parameter tokens and transformed ANF body. (Result often bound via \texttt{ir::Let}).
    \item \textbf{\texttt{DefineExpression}}: Becomes \texttt{ir::TDefine} binding name token to transformed ANF value at top level.
    \item \textbf{\texttt{DefineProcedure}}: Becomes \texttt{ir::TDefine} binding name token to an \linebreak\texttt{ir::Lambda} representing the procedure at top level.
    \item \textbf{\texttt{LetExpression}}: Equivalent nested \texttt{ir::Let} bindings for each variable/value pair, followed by the transformed body.
    \item \textbf{\texttt{SetExpression}}: \texttt{ir::Let} binding value to temp \texttt{v}, followed by \texttt{ir::App} of primitive \texttt{set!} using variable token and \texttt{v}.
    \item \textbf{\texttt{QuoteExpression}}: \texttt{ir::Quote} node containing the unevaluated quoted \texttt{Expression}.
    \item \textbf{\texttt{VectorExpression}}: Nested \texttt{ir::Let}s binding non-atomic elements to temps, ending in \texttt{ir::App} of primitive \texttt{vector} using element tokens/temps.
\end{itemize}

% --- Appendix B: ANF to TAC Transformation Rules ---
\label{app:tac-table} 

\subsubsection*{Key ANF to TAC Transformation Rules}
\label{tab:anf-to-tac-transformations-appendix}

This section outlines the general strategy for converting A-Normal Form (ANF) nodes, typically formatted as \texttt{ir::NodeName \{components\}}, into sequences of Three-Address Code (TAC) instructions. The aim is to flatten the nested ANF structure into a linear sequence of simpler TAC operations.

\begin{itemize}[itemsep=3pt]
    \item \textbf{\irnode{Let \{var, bind\_expr, body\_expr\}}}: \\
        The \texttt{bind\_expr} is first translated into a TAC sequence. If \texttt{var} is specified, its result is typically assigned to \texttt{var} using a \texttt{STORE} TAC instruction, assuming \texttt{var} represents a Scheme variable, or by ensuring the result resides in the TAC temporary associated with \texttt{var}. The \texttt{body\_expr} is then translated, and its resulting TAC value becomes the conceptual result of the entire Let expression.
    \item \textbf{\irnode{Atom \{token\}}}: \\
        An \irnode{Atom} itself usually does not generate new TAC instructions directly. Its value or name is incorporated as an operand in other TAC instructions. Should the atom be a Scheme variable needing to be fetched from memory, rather than a temporary, a \texttt{LOAD} instruction would typically be generated when it is used.
    \item \textbf{\irnode{App \{func, arg\_list\}}}: \\
        This generates TAC to evaluate each argument in \texttt{arg\_list}, ensuring results are in TAC temporaries, potentially via \texttt{LOAD} instructions. A sequence of \texttt{PARAM} instructions follows to pass these arguments. A \texttt{CALL} instruction is then emitted to the \texttt{func} target; this target might itself require a \texttt{LOAD} if \texttt{func} is a variable holding a closure. The return value from the \texttt{CALL} is usually stored in a new temporary TAC variable.
    \item \textbf{\irnode{If \{cond\_expr, then\_branch, else\_branch\}}}: \\
        This converts to TAC that evaluates \texttt{cond\_expr}. Conditional jump instructions, for example \texttt{JUMP\_IF\_NOT}, then direct control flow. \texttt{LABEL} instructions mark the TAC sequences generated for the \texttt{then\_branch} and \texttt{else\_branch}. Unconditional \texttt{JUMP} instructions ensure proper sequencing around these branches to an end label. \texttt{COPY} instructions may be used to unify the result from either branch into a single temporary TAC variable.
    \item \textbf{\irnode{Lambda \{param\_list, body\_anf\}}}: \\
        This typically generates an \texttt{ALLOC} instruction to create a runtime closure object. This object bundles a new unique label, pointing to the compiled TAC for the \texttt{body\_anf}, with a reference to the current lexical environment. The \texttt{body\_anf} itself is processed separately to generate a distinct sequence of TAC instructions, usually bracketed by \texttt{FUNC\_BEGIN} and \texttt{FUNC\_END} using this new label. The result of the \irnode{Lambda} expression is the TAC temporary holding the new closure object.
    \item \textbf{\irnode{Quote \{quoted\_expr\}}}: \\
        This translates into TAC that allocates runtime storage for the literal data structure. An \texttt{ALLOC} instruction, such as \texttt{ALLOC T\_literal, "literal", S\_literal\_data}, is typically emitted, where \texttt{S\_literal\_data} is a representation of the \texttt{quoted\_expr}. The result is a TAC temporary pointing to this allocated literal.
    \item \textbf{Top-Level \irnode{TDefine \{var\_name, value\_anf\}}}: \\
        The \texttt{value\_anf}, often an \irnode{Lambda} resulting in a closure object, is translated into TAC. A \texttt{STORE} instruction is then emitted to assign this resulting TAC value or temporary to the global \texttt{var\_name}.
\end{itemize}
\label{app:qbe-table}

\subsubsection*{Key TAC to QBE Transformation Rules.}
\label{tab:qbe-transformations-appendix}
\begin{itemize}
    \item \textbf{\texttt{COPY R, A1}}: \texttt{\%r =l copy A1}
    \item \textbf{\texttt{LABEL L}}: \texttt{@L}
    \item \textbf{\texttt{JUMP L}}: \texttt{jmp @L}
    \item \textbf{\texttt{PARAM A1}}: Argument \texttt{A1} added to list for next \texttt{CALL}.
    \item \textbf{\texttt{CALL R, F, N}}: Resolve \texttt{F} (handle closure load); Emit QBE \texttt{call} to \texttt{F} with params; Assign result to \texttt{\%r}.
    \item \textbf{\texttt{JUMP\_IF\_NOT C, L}}: \texttt{\%c =l ceql C, \$false\_obj}; \texttt{jnz \%c, @L, \@L\_next}.
    \item \textbf{\texttt{ALLOC R, "closure", Lbl}}: \texttt{\%r =l call \$make\_closure(l \$Lbl)}
    \item \textbf{\texttt{ALLOC R, "literal", S}}: Define \texttt{data \$str...}; \texttt{\%r =l call \linebreak\$allocate\_literal(l \$str...)}
    \item \textbf{\texttt{STORE G, A1}}: Calls to \texttt{\$intern\_symbol}, \texttt{loadl \$g...}, \texttt{\$env\_define(env, sym, A1)}.
    \item \textbf{\texttt{LOAD R, V}}: Calls to \texttt{\$intern\_symbol}, \texttt{loadl \$env...}, \texttt{\%r =l call \$env\_lookup(env, sym)}
    \item \textbf{\texttt{RETURN A1}}: \texttt{ret A1}
    \item \textbf{\texttt{FUNC\_BEGIN L, P}}: \texttt{function l \$L(env \%env, ...){ \@start\_L ...}}
    \item \textbf{\texttt{FUNC\_END L}}: \texttt{\}}
\end{itemize}

\section{Jaws Website Visuals}
\label{app:compiler-ex-visuals}
This section describes visuals of the Jaws Website and compiler/interpreter features. 
\begin{itemize}
    \item \textbf{\href{https://github.com/Jamie-Wales/Jaws/blob/main/assets/ANF.png}{ANF in Compiler Explorer (\texttt{ANF.png})}}: ANF showing in compiler explorer.
    \item \textbf{\href{https://github.com/Jamie-Wales/Jaws/blob/main/assets/closure.png}{Closure Example (\texttt{Closure.png})}}: Closure code example.
    \item \textbf{\href{https://github.com/Jamie-Wales/Jaws/blob/main/assets/Constant folding.png}{Constant Folding in Compiler Explorer (\texttt{Constant folding.png})}}: Constant folding showing in compiler explorer.
    \item \textbf{\href{https://github.com/Jamie-Wales/Jaws/blob/main/assets/dce.png}{DCE in Compiler Explorer (\texttt{DCE.png})}}: DCE showing in compiler explorer.
    \item \textbf{\href{https://github.com/Jamie-Wales/Jaws/blob/main/assets/letrec.png}{Letrec in Compiler Explorer (\texttt{letrec.png})}}: Letrec macro expanded in compiler explorer.
    \item \textbf{\href{https://github.com/Jamie-Wales/Jaws/blob/main/assets/Meta.png}{Meta-Circular Evaluator in Editor (\texttt{Meta.png})}}: Meta-circular evaluator running in online editor.
    \item \textbf{\href{https://github.com/Jamie-Wales/Jaws/blob/main/assets/Producer.png}{Producer-Consumer Example (\texttt{Producer.png})}}: The producer-consumer problem demonstrating Jaws' concurrency features.
    \item \textbf{\href{https://github.com/Jamie-Wales/Jaws/blob/main/assets/Snake_game.png}{Snake Game Implementation (\texttt{Snake\_game.png})}}: Snake Implementation, showcasing an FFI example program.
    \item \textbf{\href{https://github.com/Jamie-Wales/Jaws/blob/main/assets/swap.png}{Swap Macro in Compiler Explorer (\texttt{swap.png})}}: Swap macro showing in compiler explorer.
    \item \textbf{\href{https://github.com/Jamie-Wales/Jaws/blob/main/assets/TAC.png}{TAC in Compiler Explorer (\texttt{TAC.png})}}: TAC conversion shown in compiler explorer.
    \item \textbf{\href{https://github.com/Jamie-Wales/Jaws/blob/main/assets/Tailrecursion.png}{Tail Recursion Example (\texttt{TailRecursion.png})}}: Tail recursion example.
\end{itemize}

\section{Interpreter Implementation Details}
\label{app:interpreter-details}

This appendix provides supplementary details about the interpreter's implementation, specifically regarding special forms and runtime value representation.

\subsubsection*{Scheme Special Forms and Evaluation Rules.}
\label{tab:special-forms-appendix}
\begin{itemize}
    \item \textbf{\texttt{if}}: Evaluates only the condition expression. Based on the result (\texttt{\#t} or \texttt{\#f}), evaluates \emph{only} the corresponding \texttt{then} or \texttt{else} branch.
    \item \textbf{\texttt{define}}: Evaluates the value expression. Binds the specified identifier to the resulting value in the \emph{current} lexical environment. Does not evaluate the identifier name itself.
    \item \textbf{\texttt{set!}}: Evaluates the value expression. Finds an \emph{existing} binding for the identifier in the current or an ancestor environment and updates it to the resulting value. Throws an error if the identifier is unbound.
    \item \textbf{\texttt{quote}}: Prevents evaluation of its operand. Returns the operand expression itself, treated as data for example a symbol or list structure.
    \item \textbf{\texttt{quasiquote}}: Similar to \texttt{quote}, but allows specific parts within the quoted structure to be evaluated using \texttt{unquote} and \texttt{unquote-splicing}.
    \item \textbf{\texttt{unquote}}: (Within \texttt{quasiquote}) Evaluates its operand and substitutes the result back into the quasiquoted structure.
    \item \textbf{\texttt{unquote-splicing}}: (Within \texttt{quasiquote}) Evaluates its operand, which must result in a list. Splices the elements of this list into the surrounding quasiquoted list structure.
    \item \textbf{\texttt{let}}: (Often syntactic sugar) Creates new bindings in a new lexical scope and evaluates the body within that scope. Handled specially to manage environment creation correctly.
    \item \textbf{\texttt{define-syntax}}: Defines a macro. Handled entirely by the macro expansion phase, not the interpreter's evaluation phase.
\end{itemize}

\subsubsection*{Runtime Value Representations (\texttt{SchemeValue}).}
\label{tab:runtime-values-appendix}
\begin{itemize}
    \item \textbf{Number (Numeric Tower)}: A dedicated \texttt{Number} class using \texttt{std::variant} internally to manage integers, rationals, floats, and complex numbers accurately.
    \item \textbf{Boolean (\texttt{\#t}, \texttt{\#f})}: Directly mapped to the host language's native \texttt{bool} type.
    \item \textbf{Pair / List / Dotted Pair}: Implemented using \texttt{std::shared\_ptr} to a heap-allocated \texttt{std::list<SchemeValue>}. The canonical empty list \texttt{()} is represented by a unique shared pointer instance. Dotted pairs are handled by convention within this list structure.
    \item \textbf{Vector}: Implemented using \texttt{std::shared\_ptr} to a heap-allocated \newline\texttt{std::vector<SchemeValue>}, providing efficient random access.
    \item \textbf{Procedure (User-defined / Closure)}: A \texttt{UserProcedure} class derived from \texttt{Procedure} holds parameter names, body expressions, and crucially, a \newline\texttt{std::shared\_ptr} to the captured lexical \texttt{Environment}. The overall procedure is stored via \texttt{std::shared\_ptr<Procedure>}.
    \item \textbf{Procedure (Built-in)}: A \texttt{BuiltinProcedure} class derived from \texttt{Procedure} wraps a C++ \texttt{std::function}, allowing direct execution of primitive operations. Stored via \texttt{std::shared\_ptr<Procedure>}.
    \item \textbf{Continuation}: A \texttt{Continuation} class derived from \texttt{Procedure} encapsulates a captured \texttt{InterpreterState}, including the environment chain and control state, enabling non-local control flow via \texttt{call/cc}. Stored via \newline\texttt{std::shared\_ptr<Procedure>}.
    \item \textbf{String}: Directly mapped to the host language's \texttt{std::string} type.
    \item \textbf{Character}: Directly mapped to the host language's native \texttt{char} type.
    \item \textbf{Symbol}: Represented by a lightweight \texttt{Symbol} struct containing the identifier name as a \texttt{std::string}.
    \item \textbf{Port (Input/Output/Socket)}: A \texttt{Port} class acts as an abstraction layer, wrapping either a standard C++ stream or a custom socket stream object, managed via \texttt{std::shared\_ptr}.
    \item \textbf{Multiple Values}: A dedicated \texttt{MultiValue} struct, holding a \newline\texttt{std::vector<SchemeValue>}, is used to support procedures returning multiple values via \texttt{values} and \texttt{call-with-values}, stored via \texttt{std::shared\_ptr}.
    \item \textbf{Thread Handle}: A \texttt{ThreadHandle} struct containing the native \texttt{std::thread}, completion status, result storage, and mutex, managed by \texttt{std::shared\_ptr}.
    \item \textbf{Mutex}: A \texttt{MutexHandle} struct wrapping a native \texttt{std::mutex}, managed by \texttt{std::shared\_ptr}.
    \item \textbf{Condition Variable}: A \texttt{ConditionVarHandle} struct wrapping a native \newline\texttt{std::condition\_variable}, managed by \texttt{std::shared\_ptr}.
    \item \textbf{Unevaluated Expression}: For internal use by \texttt{quote}, a \newline\texttt{std::shared\_ptr<Expression>} is held directly within the \texttt{SchemeValue}'s variant.
\end{itemize}



\section{Key Jaws Project Source Files}
\label{app:jaws-source-files}

The complete Jaws project, including all source code files detailed in this appendix, is publicly available at: \url{https://github.com/jamie-wales/Jaws}.
This appendix outlines key C++ source code files for the Jaws Scheme interpreter and compiler, categorised by their primary role in the system. Links point to the main branch of the project repository.

\subsection{Core Interpreter and Frontend Components}

\subsubsection*{Interpreter and Frontend Source Files.}
\label{tab:jaws-interpreter-frontend-files}
\begin{itemize}
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/main.cpp}{main.cpp}}: Main entry point for Jaws. Handles CLI arguments, initializes the REPL or script execution.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/frontend/scan.cpp}{scan.cpp}}: Lexical analyser converts Scheme source text into a sequence of tokens.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/frontend/parse.cpp}{parse.cpp}}: Parser, constructs an Abstract Syntax Tree (AST) from the token stream.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/frontend/Expression.cpp}{Expression.cpp}}: Defines AST node structures representing Scheme expressions.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/frontend/MacroTraits.cpp}{MacroTraits.cpp}}: Core logic for Scheme's hygienic macro system, including pattern matching and template expansion.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/frontend/MacroEnvironment.cpp}{MacroEnv.cpp}}: Manages environments for macro definitions and their expansion contexts.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/frontend/Environment.cpp}{Environment.cpp}}: Implements Scheme's lexical runtime environments for variable and procedure bindings.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/interpreter/import.cpp}{Import.cpp}}: Handles `import` declarations for loading Scheme libraries according to R7RS.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/interpreter/interpret.cpp}{Interpret.cpp}}: Core tree-walking interpreter engine for evaluating AST nodes.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/interpreter/Value.cpp}{Value.cpp}}: Defines the runtime `SchemeValue` class for representing all Scheme data types.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/interpreter/Procedure.cpp}{Procedure.cpp}}: Implementation for user-defined procedures, closures, and built-in procedures.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/interpreter/Number.cpp}{Number.cpp}}: Handles Scheme's numeric tower (integers, rationals, floats, complex numbers).
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/interpreter/FFI.cpp}{FFI.cpp}}: Implements the Foreign Function Interface for interoperability with C code.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/interpreter/Port.cpp}{Port.cpp}}: Implements I/O ports for file and string operations, including socket abstractions.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/interpreter/SocketStream.cpp}{SocketStream.cpp}}: Provides low-level socket stream functionality used by networking ports.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/interpreter/builtins/}{builtins/}}: Directory containing C++ implementations for Scheme built-in procedures including, math, lists, I/O, strings, equality, HOFs, values, threads, FFI bindings.
\end{itemize}
\subsection{C Runtime System Components}
The following list outlines the C source files that constitute the Jaws runtime system. This system is responsible for memory management, including garbage collection, Scheme object representation, environment handling, and primitive operations called by compiled QBE code. 


\subsubsection*{C Runtime System Source Files.}
\label{tab:jaws-runtime-files}
\begin{itemize}
\item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/gc.c}{gc.c}}: Implements the Mark-Sweep-Compact garbage collector, heap allocation (alloc\_object, allocate), and runtime initialisation (init\_runtime, cleanup\_runtime). Manages heap memory, performs root marking (including conservative stack scanning), and object compaction.
\item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/env.c}{env.c}}: Handles Scheme environments (SchemeEnvironment), including creation (new\_environment), variable lookup (env\_lookup), and definition (env\_define). It also manages the global symbol table (intern\_symbol, init\_symbol\_table) and initialises the global runtime environment (init\_global\_environment). Implements a hash map for bindings and the symbol table.
\item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/types.c}{types.c}}: Defines the SchemeObject structure and SchemeType enumeration. Provides functions for creating closures/functions (make\_closure, make\_function) and converting Scheme objects to strings for display (to\_string, to\_string\_pair).
\item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/jaws_io.c}{jaws\_io.c}}: Contains implementations for basic I/O primitive operations callable from compiled code, such as display and newline. Also includes example arithmetic primitives like plus and multiply that operate on SchemeObjects.
\item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/jaws_list.c}{jaws\_list.c}}: Implements the fundamental list constructor cons, which allocates and initialises a SchemeObject of type TYPE\_PAIR.
\item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/jaws_eq.c}{jaws\_eq.c}}: Provides an implementation for the null? predicate (is\_null) which checks if a Scheme object is nil.
\end{itemize}
\subsection{Compiler Pipeline and Utility Components}
The following list outlines the files related to the Scheme compiler, including intermediate representation transformations, optimisations, and code generation, as well as general utilities. 

\subsubsection*{Compiler Pipeline and Utility Source Files.}
\label{tab:jaws-compiler-utility-files}
\begin{itemize}
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/compiler/ANFTransformer.cpp}{ANFTransformer.cpp}}: Converts Scheme AST to A-Normal Form (ANF) for the compiler pipeline.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/compiler/ANF.cpp}{ANF.cpp}}: Defines the data structures for the A-Normal Form (ANF) intermediate representation.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/compiler/ConstantFold.cpp}{ConstantFold.cpp}}: Implements the constant folding optimisation pass on the ANF representation.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/compiler/DeadCodeElimination.cpp}{DCE.cpp}}: Implements the dead code elimination (DCE) optimisation pass on the ANF representation.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/compiler/optimise.cpp}{optimise.cpp}}: Orchestrates various optimisation passes for the compiler on the ANF IR.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/compiler/ThreeAC.cpp}{ThreeAC.cpp}}: Defines Three-Address Code (TAC) structures and implements the conversion from ANF to TAC.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/compiler/QBEGenerator.cpp}{QBEGenerator.cpp}}: Generates QBE (Quick Backend) intermediate language code from Three Address Code.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/utils/run.cpp}{run.cpp}}: Utility functions for orchestrating the execution of the interpreter or compiler pipeline, file I/O, and argument parsing.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/src/utils/ExpressionUtils.cpp}{ExpressionUtils.cpp}}: Helper functions for constructing, manipulating, or converting AST `Expression` nodes.
\end{itemize}
\subsection{Scheme Standard Library (R7RS) Components}
The Jaws Scheme standard library, implemented in Scheme itself according to R7RS principles, provides a rich set of core functionalities, utilities, and mathematical operations. These files are located in the `lib/` directory of the project.

\subsubsection*{Scheme Standard Library Source Files.}
\label{tab:stdlib}
\begin{itemize}
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/lib/base.scm}{base.scm}}: Defines fundamental R7RS syntax such as, cond, let*, and, or, define-values. As well as core procedures like, not, print, println, zero?.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/lib/list-utils.scm}{list-utils.scm}}: Provides a comprehensive suite of R7RS list manipulation utilities including car/cdr combinations such as cadr, cddr, caar. Other key utilities such as length, reverse, list-tail, memq, memv, member, assoc, assq, assv, last, and last-pair.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/lib/loops.scm}{loops.scm}}: Implements various extended looping constructs like while, for, repeat, until, for-each, for-range, iterate, and select-case.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/lib/math.scm}{math.scm}}: Contains R7RS mathematical procedures and constants, including arithmetic (+, -, *, /), comparisons (<, =, >), number type predicates integer, real?, abs, sqrt, gcd, lcm, trigonometric functions (sin, cos, atan), exp, log, and constants pi, e.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/lib/utilities.scm}{utilities.scm}}: Offers higher-order functions and general programming utilities such as filter, fold-left, fold-right, sort, any?, every?, find, find-tail, remove, partition, compose, identity, and constantly.
    \item \textbf{\href{https://github.com/jamie-wales/Jaws/blob/main/lib/vector-utils.scm}{vector-utils.scm}}: Provides utility functions for vector operations like vector-map, vector-for-each, and vector-append.
\end{itemize}

\subsection{Non-Trivial Examples}

\label{tab:non-trivial}
\begin{itemize}
    \item \textbf{Producer Consumer}: Non trivial treading code can be found \href{https://github.com/jamie-wales/Jaws/blob/main/producer.scm}{here}, and output screenshot can be found \href{https://github.com/jamie-wales/Jaws/blob/main/assets/Producer.png}{here}.
    \item \textbf{Snake}: Non trivial snake code can be found at \href{https://github.com/Jamie-Wales/Jaws/blob/main/ffi_test/raylib.scm}{snake}. A screenshot of the output can be found \href{https://github.com/jamie-wales/Jaws/blob/assets/Snake_game.png}{here}
    \item \textbf{Meta}: Meta circular evaluator code can be found at \href{https://github.com/jamie-wales/Jaws/blob/meta.scm}{meta}. A visual example of this running can be found at \href{https://github.com/Jamie-Wales/Jaws/blob/main/assets/Meta.png}{Meta}

\end{itemize}



\section{Core Algorithms}
\label{app:core-algorithms}

This section provides pseudocode for key algorithms used in the interpreter and compiler.

\label{app:trampoline}


\begin{algorithm} 
\caption{Trampolining Algorithm for Tail Call Optimisation}
\label{alg:trampoline}
\begin{algorithmic}[1]
\Function{ExecuteProcedure}{initial\_state, initial\_procedure, initial\_arguments}
    \State state $\gets$ initial\_state
    \State currentProc $\gets$ initial\_procedure
    \State currentArgs $\gets$ initial\_arguments
    \Loop
        \If{currentProc is a \texttt{UserProcedure} and state indicates pending tail call}
            \State Load pending procedure into currentProc from state
            \State Load pending arguments into currentArgs from state
            \State Reset tail call state
            \State \textbf{continue} 
        \EndIf 
        \State result $\gets$ \Call{ApplyProcedure}{state, currentProc, currentArgs} 
        \If{state indicates tail call was requested during ApplyProcedure}
            \State \textit{// state holds next proc/args, loop will handle it}
        \Else
            \State \Return result 
        \EndIf 
    \EndLoop 
\EndFunction
\end{algorithmic}
\end{algorithm}

\label{app:recursive-descent}

\begin{algorithm} 
    \caption{Recursive Descent Parser for Scheme}
\label{alg:recursive-descent}
\begin{algorithmic}[1]
\Function{Parse}{token stream}
    \State Initialise \texttt{ParserState} (current token index)
    \State \Return \Call{ParseExpression}{}
\EndFunction 

\Function{ParseExpression}{}
    \State token $\gets$ \Call{CurrentToken}{}
    \If{token is \texttt{LEFT\_PAREN}}
        \If{token is \texttt{LET}} 
            \State \Return \Call{ParseLet}{}
        \ElsIf{token is \texttt{Define}} 
            \State \Return \Call{ParseDefine}{}
        \ElsIf{token is \texttt{\#(}} \Comment{Vector literal}
            \State \Return \Call{ParseVector}{}
        \Else 
            \State Report error: Unexpected form or unhandled s-expression start
        \EndIf 
    \ElsIf{token is ATOM (literal or identifier)} 
        \State \Return \Call{ParseAtom}{}
    \Else
        \State Report error: Unexpected token
    \EndIf 
\EndFunction 
\end{algorithmic}
\end{algorithm}

\label{app:dce}

\begin{algorithm} 
\caption{Dead Code Elimination using Dependency Graph}
\label{alg:dce}
\begin{algorithmic}[1]
\Function{PerformDCE}{TopLevel ANF program $P$}
    \State $G \gets$ \Call{ConstructDependencyGraph}{$P$} \Comment{Build graph from ANF}
    \State $Roots \gets$ \Call{GetSideEffectNodes}{$G$} $\cup$ \{"root"\} \Comment{Find essential nodes}
    \State $LiveNodes \gets \emptyset$
    \State $WorkList \gets$ queue initialised with $Roots$
    \While{$WorkList$ is not empty}
        \State $node \gets$ \Call{Dequeue}{$WorkList$}
        \If{$LiveNodes$ does not contain $node$}
            \State Add $node$ to $LiveNodes$
            \For{each predecessor $pred$ of $node$ in $G$} \Comment{Work backwards}
                \State \Call{Enqueue}{$WorkList$, $pred$}
            \EndFor 
        \EndIf 
    \EndWhile 
    \State $P' \gets$ \Call{FilterProgram}{$P$, $LiveNodes$} \Comment{Keep only live top-level defs}
    \State \Return $P'$
\EndFunction
\end{algorithmic}
\end{algorithm}

\label{app:const-fold}

\begin{algorithm} 
\caption{Constant Folding for ANF Expressions (Conceptual)}
\label{alg:const-fold}
\begin{algorithmic}[1]
\Function{ConstantFoldRecursive}{ANF node $N$, ConstMap $M$}
    \If{$N$ is \texttt{ir::Atom}$(token)$}
        \If{$token$ represents a constant $C$}
            \State \Return $N$.
        \EndIf 
        \If{$M$ contains $token.lexeme$}
            \State \Return \texttt{ir::Atom} node for $M[token.lexeme]$.
        \EndIf 
        \State \Return $N$.
    \ElsIf{$N$ is \texttt{ir::App}$(func, args)$}
        \If{\texttt{IsPurePrimitive}$(func)$ and all $args$ resolve to constants $C_1, ..., C_n$ using $M$}
            \State $Result \gets$ \Call{EvaluatePrimitive}{$func$, $C_1, ..., C_n$}
            \State \Return \texttt{ir::Atom} node for $Result$.
        \EndIf 
        \State \Return $N$. \Comment{Cannot fold or not pure}
    \ElsIf{$N$ is \texttt{ir::Let}$(name, binding, body)$}
        \State $foldedBinding \gets$ \Call{ConstantFoldRecursive}{$binding$, $M$}
        \State $M_{new} \gets M$
        \If{$name$ exists and $foldedBinding$ evaluates to constant $C$}
            \State $M_{new}[name] \gets C$
        \EndIf 
        \State $foldedBody \gets$ \Call{ConstantFoldRecursive}{$body$, $M_{new}$}
        \State \Return \texttt{ir::Let}$(name, foldedBinding, foldedBody)$. \Comment{Rebuild Let}
    \Else \Comment{\texttt{ir::Lambda}, \texttt{ir::Quote}}
        \State \Return $N$. 
    \EndIf 
\EndFunction 
\end{algorithmic}
\end{algorithm}

\end{document}
