%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  FINAL REPORT - A TEMPLATE
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[final]{cmpreport_02}


% Some package I am using. You may not need them
%
\usepackage{rotating}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily,
  keywordstyle=\color{blue},
  commentstyle=\color{green!60!black},
  stringstyle=\color{red},
  showstringspaces=false,
  numbers=left,
  numberstyle=\tiny,
  numbersep=5pt,
  frame=single,
  breaklines=true
}
\usepackage{subfloat}
\usepackage{color}
\usepackage{pdfpages}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

%\setkeys{Gin}{draft}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  Fill in the fields with:
%
%  your project title
%  your name
%  your registration number
%  your supervisor's name
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\title{Jaws: Jaws Awesomely Wrangles Scheme}
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% The author's name is ignored if the following command 
% is not present in the document
%
% Before submitting a PDF of your final report to the 
% project database you may comment out the command
% if you are worried about lack of anonimity.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\author{Jamie Michael Wales}
\registration{100067069}
\supervisor{Dr. Rudy Lapeer}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Fill in the field with your module code.
% this should be:
%
% for BIS project module   -> CMP-6012Y
% for CS project module    -> CMP-6013Y
% for MComp project module -> CMP-7043Y
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\ccode{CMP-6013Y}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Comment out if confidential report.
% The command should be used if the project is subjected 
% to a Non Disclosure Agreement.
%
% Three examples of the use of the \confidential command. 
% Please ask your supervisor what confidential statement 
% should be used, if appropriate.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\confidential{}

%\confidential{The contents of this report remain confidential for two years and should not be discussed or disclosed to any third party without the prior written permission from the School of Computing Sciences, the University of East Anglia}

%\confidential{The information contained in this document is confidential, privileged and only for the information of the intended recipient and may not be used, published or redistributed without the prior written consent of FruitName Ltd}

\summary{
The abstract of your report summarises your entire work (and as such, your report) in no more than half a page. It should include the context of your work including its main objective, what methods you employed, how you implemented these, what the outcomes were and a final statement as a conclusion. It should not contain acronyms, abbreviations, elements of a literature review (though a statement of related work is permissible if it is crucial to your work) or future work. The abstract should be written when everything else has been written up and the project is finished!
}

\acknowledgements{
Thank you to Dr. Rudy Lapeer, who helped channel my enthusiasum into a well rounded academic project. Thank you to Ella my partner who is the glue that holds my life together. To my brother, who will always listen to me go on a tangent about compilers and to my parents for their love and support.
}

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% If you do want a list of figures and a list of tables
% to appear after the table of contents then comment this line.
% THIS IS NOT ADVISED THOUGH AS IT COUNTS FOR YOUR 40 PAGES!
%
% Note that the class file contains code to avoid
% producing an empty list section (e.g list of figures) if the 
% list is empty (i.e. no figure in document).
%
% The command also prevents inserting a list of figures or tables 
% anywhere else in the document
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\nolist

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
% Comment out if you want your list of figures and list of
% tables on one page instead of two or more pages, in particular 
% if the lists do not fit on a single page.
%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%\onePageLists


\begin{document}

\section{Introduction}

This section is a "must have" in the report. It should not be longer than a couple of pages but has to  clearly present the problem statement and main objective or aim of the project, followed by the potential solutions and the specific objectives that are part of them. Use a subsection for your main objective and another subsection to specify your sub-objectives in a bullet-pointed list. You may wish to put a MoSCoW at the end of this section (or in the appendix), if appropriate, but if you do so then make sure it matches your objectives and you do come back to it in the final section that briefly discusses and concludes your work.

\section{Background}

Another section that is essential and should keep its title as is! Although you could perhaps call it ``Literature Review'' instead, this is not advisable as at this stage of your project we do not expect an extensive literature review since this was already done in the second formative assignment. The rationale is simply because you will lose valuable pages that could be used better in the next two sections that will cover the preparation and implementation of actual work done. So just provide the context in which your project operates here, and then provide a brief overview of similar work that is directly relevant to yours. Try to avoid blatant copying and pasting from the formative literature review as it is bound to read awkwardly.

\section{Methodology}

The way in which this project was approached was in three seperate sections. First, develop a interpreter for the scheme programming language. Second, develop a compiler for the scheme programming language. Third, develop a web-based interface for the scheme programming language.
From the outset it was clear that there was a huge amount of work to be done in order to complete the project. To give the project the best chance of developing every part a standard tree-walk interpreter approach was selected for the interpreter.
A tree-walk interpreter is the simplest of the approaches to interpretation where we walk an abstract syntax tree evaluating it as we process it. This approach was selected as it is the simplest to implement and would allow for the development of the compiler and web-based to be developed thereafter.
It is important to note that a compiler, would use the same front-end as an interpreter but split it's pipeline off. The compiler would generate some sort of intermediate representation (IR) which would then be converted into machine code.

\subsection{Interpreter}

\subsubsection{Lexer}
The initial development of any compiler/interpreter project begins with establishing an effective front-end strategy. This critical first phase involves selecting appropriate approaches for lexing and parsing to transform raw text into a structured format.
For lexical analysis, several methodologies were considered, including, manual character-by-character scanning with explicit state management. Using lexer generator tools like Flex or ANTLR; Or finally, Regular expression-based pattern matching
The regular expression approach was selected for several key reasons. 
First, regular expressions naturally map to the deterministic finite automaton (DFA) model described by Cooper and Torczon (ADD REF) for lexical analysis. Second, they offer a more concise and maintainable way to express token patterns compared to manual character processing. Third, they avoid the additional learning curve and external dependencies associated with lexer generators, keeping the project self-contained.
This methodology involves organising token patterns in a prioritised list, with more specific patterns (like keywords) checked before more general ones (like identifiers). This prioritisation is crucial for correct tokenization since patterns may overlap. For instance, scheme-specific patterns for complex numbers needed to be checked before simpler numeric patterns to ensure proper recognition.
For keyword handling, a two-stage approach was selected: first identify all text as a general identifier, then check a keyword map to determine if it's actually a reserved word. This approach simplifies the regex patterns while maintaining the language's semantics.
The complete set of token patterns and their priorities were determined by analysing the R7RS Scheme specification (ADD REF), ensuring comprehensive coverage of the language syntax.

\subsubsection{Parser}
Parsing naturally follows lexing in the construction of an interpreter. The academic literature on parsing strategies is extensive, (ADD REF "Dragon Book") dedicating significant coverage to the topic. 
As with lexing, there exist tools that generate parsers automatically, typically producing table-based implementations that would be prohibitively complex to write manually. For this project, a recursive descent parsing (Algorithm) approach was selected, leveraging Scheme's homoiconic nature.
The extensive amount of parsing strategies in compiler theory largely stems from the complexity of handling operator precedence. Standard programming languages employ various operator types: postfix (after the operands), prefix (before the operands), infix (between operands), and even mixfix (such as the ternary operator, which interleaves with its operands). Different parsing techniques attempt to efficiently distinguish between these forms.
However, much of the literature on parsing optimisation originates from an era when computational efficiency was more constrained. Modern hardware has somewhat diminished these concerns relative to implementation simplicity and maintainability.
Furthermore, Scheme's homoiconic syntax, where every operation is uniformly expressed with parentheses in prefix notation. This significantly simplifies the parsing challenge. Ultimately, it eliminates the need for complex precedence handling, making recursive descent parsing a straightforward and appropriate methodology for this project.

\subsubsection{Macros}
While most programming languages proceed directly from parsing to interpretation, Scheme introduces a crucial intermediate step: macro expansion. Scheme's powerful macro system represents one of its most distinctive features, enabling sophisticated syntactic extensions that transform the Abstract Syntax Tree before evaluation begins.

Unlike traditional C-style preprocessor macros that operate through simple text substitution, Scheme implements a hygienic macro system that works with structured expressions. This methodological approach provides significant advantages:

\begin{itemize}
    \item Elimination of variable capture problems and unintended evaluation order issues
    \item Pattern-matching capabilities that recognize and transform complex syntax patterns
    \item Preservation of lexical scoping during expansion
    \item The ability to create domain-specific language extensions within Scheme itself
\end{itemize}

The implementation for this project follows the R7RS specification's syntax-rules system, which defines macros as pairs of patterns and templates. When a macro is invoked, the system matches the invocation against defined patterns sequentially until finding a match, then generates the corresponding template with appropriate substitutions.

For example, the following macro definition creates an intuitive 'for' loop syntax that expands to a map operation:

\begin{figure}[h!]
\caption{Example Scheme Macro Definition}
\begin{lstlisting}[language=Lisp]
(define-syntax for
  (syntax-rules (in as)
    ((for element in list body ...)
     (map (lambda (element)
            body ...)
          list))
    ((for list as element body ...)
     (for element in list body ...))))
\end{lstlisting}
\end{figure}

This macro enables more readable code like this:

\begin{figure}[h!]
\caption{Example Macro Usage}
\begin{lstlisting}[language=Lisp]
(for x in '(1 2 3 4)
     (display x)
     (* x x))

(* This expands to: *)
(map (lambda (x)
       (display x)
       (* x x))
     '(1 2 3 4))

\end{lstlisting}
\end{figure}

To ensure proper hygiene, the implementation required a separate evaluation environment for macro expansion. This methodology involves tracking the origin of each identifier and performing appropriate renaming during expansion, preventing variables in the macro definition from capturing variables at the macro use site.
The macro expansion process transforms the initial AST into an expanded form where all macros have been replaced with their expansions. This approach aligns with the classic Scheme philosophy of providing a minimal core language with powerful syntactic extensions built on top, effectively creating an extended language that is expressed in terms of simpler core constructs.

\subsubsection{Evaluation}
Once an AST has been developed and processed through macro expansion, the evaluation phase begins. This phase is where the Scheme language's semantics are applied to execute the program. The R7RS Scheme specification defines several key requirements that any compliant implementation must satisfy.

Scheme's evaluation model requires lexical scoping, where variables are resolved in the environment where they were defined, not where they are used. This contrasts with dynamic scoping found in some other languages and necessitates a hierarchical environment structure. Additionally, Scheme treats procedures as first-class values that can be passed as arguments, returned from functions, and stored in data structures, requiring the evaluation system to handle functions as ordinary values.

The language specification also defines special forms such as \texttt{if}, \texttt{lambda}, and \texttt{define} that follow unique evaluation rules different from regular function calls.To clarify why define must be a special form in Scheme: If define were implemented as a regular procedure call, the interpreter would evaluate its arguments before applying the procedure, following standard applicative-order evaluation. This would cause the interpreter to evaluate the variable name before it's defined, leading to errors or incorrect bindings. Additionally, the environment in which definitions occur is crucial - define must create bindings in the current lexical environment rather than in the evaluation environment of a procedure call. These requirements necessitate that define be implemented as a special form with its own unique evaluation rules that differ from regular procedure calls, allowing it to properly manipulate the environment structure according to Scheme's lexical scoping rules. For instance, conditional expressions only evaluate the necessary branches, and lambda expressions create closures without evaluating their bodies. Perhaps most importantly, the R7RS specification mandates tail call optimisation, requiring implementations to optimise tail calls to prevent stack overflow in recursive procedures. This optimization allows recursive procedures to execute in constant space when the recursion occurs in tail position.

The evaluation process uses a dispatch mechanism that selects the appropriate evaluation strategy based on the node type. Self-evaluating forms like numbers and strings simply return their value. Variable references are resolved by searching the environment chain. Special forms are handled by dedicated evaluation functions that implement their specific rules. For example, the \texttt{lambda} evaluator creates a closure object containing both the function definition and a reference to the current environment, capturing the lexical context.

The requirement for tail call optimisation presented a significant challenge, particularly when implementing in a host language that doesn't natively support this feature. To address this, a technique known as "trampolining" \ref{alg:trampoline}was selected. When a procedure call is identified as being in tail position (the final expression to be evaluated in a function body or conditional branch), instead of executing the call directly, the evaluator returns a tailcall object containing the procedure and arguments. The main evaluation loop detects these tailcall objects and executes them iteratively without growing the call stack. This effectively transforms recursive processes into iterative ones at the implementation level.

\subsubsection{Values}
During interpretiation objects must be created to represent the values that are being manipulated. These objects need represent the many different types as per the RSR7 specification. These objects are as follows:
\begin{itemize}
    \item \textbf{Number}: Scheme's numeric tower includes integers, rationals, and floating-point numbers. The interpreter must support all these types and provide appropriate arithmetic operations for each.
    \item \textbf{String}: Scheme strings are sequences of characters enclosed in double quotes. The interpreter must provide operations for creating, accessing, and manipulating strings.
    \item \textbf{Character}: Scheme has a distinct character type for representing individual characters. The interpreter must provide operations for creating, comparing, and manipulating characters.
    \item \textbf{Symbol}: Symbols are unique identifiers used to represent variables, functions, and other named entities in Scheme. The interpreter must provide operations for creating, comparing, and interning symbols.
    \item \textbf{Boolean}: Boolean values in Scheme are \texttt{\#t} for true and \texttt{\#f} for false. The interpreter must provide operations for creating, comparing, and manipulating boolean values.
    \item \textbf{Pair}: The fundamental data structure in Scheme, pairs are used to construct lists and other compound data structures. The interpreter must provide operations for creating, accessing, and manipulating pairs.
    \item \textbf{Vector} Vectors are fixed-length sequences of elements that can contain any type of value. The interpreter must provide operations for creating, accessing, and manipulating vectors. This includes bytevectors which are vectors of bytes.
    \item \textbf{Procedure} Procedures in Scheme are first-class values, the interpreter needs to treat these as such. Procedures need to store their own environment of symbboles and values. As well as constructing closures, which are procedures that return procedures, that have access to their parents scope.
    \item \textbf{Port}: Ports are schemes abstraction around input and output. The interpreter must provide operations for creating, reading, and writing to ports.
    \item \textbf{Continuations}: Continuations are a powerful feature of Scheme that allow for the capture and reification of the current state of the program. The interpreter must provide operations for creating, capturing, and invoking continuations.
\end{itemize}

Scheme is dynamically typed, which means an objects type is deterimined at runtime. Therefore the interpreter must be able to handle the many different types of objects that can be created. 
Approaches on how to handle this within the host language differ (ADD REF). There are three main approaches, these are bloated structs, tagged unions/polymorphic variants, tagged pointers. 
Bloated structs refer to a structure that holds all the possible values that an object can hold. This is the most memory intensive approach but is the fastest. Tagged unions are a more memory efficient approach but are slower than bloated structs. Tagged pointers are the most memory efficient approach but are the slowest.
During implementation polymorphic variants were selected as the approach to handle the many different types of objects that can be created, whilst maintaining type safety and satisfying the host langauges type system. 
This approach was selected as it is middle ground between tagged pointers and bloated structs, and makes imeplementation easier and more maintainable.

\subsection{Compiler}
A major part of this project was working toward a compiler for the Scheme programming language. Modern are complex programs in their own right. In order to understand the complexity of these programs, two intermediate steps we're delivered.
First was transforming the AST into an intermediate representation (IR). This IR is a data structure that represents the program in a way that is easier to manipulate and transform. The second was to perform to perform performance optimisations on these representations.
The intermediate representation (IR) is the core data structure in a compiler, serving as the central model of the program being compiled. Most compiler components interact with this IR, reading from it and modifying it. Therefore, the design choices about what information to include in the IR and how to structure it significantly impact both compilation efficiency and the quality of the resulting code. \cite{cooper2011engineering}
The intermediate representations chosen for the project we're A Normal Form (ANF) and Three Address Code (TAC). For ANF the optimisation chosen is dead code elimination and constant folding. For TAC the optimisation chosen was scope hoisting and inline definition.

\subsubsection{A Normal Form}
[write some shit here jamie]

\subsubsection{Three Address Code}
[write some shit here jamie]




This is my preferred title for the section that follows the background but it may not work for all types of projects, in particular if your methodology is more related to planning and/or design. Either way, this section falls in the scope of \textbf{preparing your project} for action and where you list all the \textbf{methods, algorithms, tools, plans and designs} that you will need later on, as discussed in the next section. As also outlined in the project portfolio brief, one or more of the following is what would be covered in this section:

\begin{itemize}
\item \textbf{Methodology: } Explanation (and justification) of methods, algorithms (typically written in pseudo-code), mathematical or statistical models, technologies etc.\ that you will implement as part of your project. These may come from other sources (e.g.\ the literature, Github, etc.) or be your own creation. Note that it should not cover methods that you will not use! If these are worthwhile mentioning then briefly discuss them in the Background section instead.
\item \textbf{Design: } Design of experiments, design of a survey or design of a system that consists of multiple components e.g. software (use preliminary diagrams to describe the design) or a physical manifestation such as an embedded system, a robot, etc.
\item \textbf{Plan(ning): } Gathering of data, description of experiments (experimental plan), testing and evaluation planning. Experiments could be in the fields of data science, machine learning, signal processing, graphics, etc. Evaluation metrics could include performance speed, accuracy, relevance, etc. Although evaluation is part of the next section, evaluation \textbf{metrics} should be explained here.
\end{itemize}

\section{Implementation and Evaluation}

Could be a section each for implementation and evaluation if this suits you better or you could use subsections instead. The difference between this section and the previous "Methodology" section is that this one covers "action" or in other words your active contributions to the project. These may include:
\begin{itemize}
\item Implementation of programming code: Describe your final code architecture using for example (UML) diagrams and code snippets. Make sure that code snippet (figure) captions are self-explanatory which means that you should not have to consult the text body to understand what is shown in the figure. Many code snippets of the same kind should end up in an appendix instead.
\item Results from experiments run, including testing (user and software). Use figures and tables with self-explanatory captions (see earlier statement). Multiple figures and tables that cover several pages should be put in an appendix.
\item Analysis of results: Discuss your experimental and/or test findings in depth. Compare them against other studies and/or benchmarks, and point out limitations of your work (that could be due to limited time) and elaborate on scope for improvement.
\end{itemize}

\section{Conclusion and Future Work}

Another essential section that should keep its title as suggested. Briefly discuss your main findings, outcomes, results; what worked and what could have been done differently. Then summarise your work in a concluding statement by comparing your outcomes against the main and sub-objectives and/or MoSCoW requirements (if used) and suggest potential future work that could be done if more time would be available.


\clearpage

\bibliography{reportbib}

\appendix
\clearpage
\section{Lexer Implementation Details} \label{app:regex}

\subsection{Regular Expression Patterns}
The lexer for the Scheme interpreter utilises the following regular expression patterns to identify tokens:

\begin{table}[h]
\caption{Regular Expression Patterns for Scheme Lexer}
\label{tab:regexpatterns}
\begin{tabular}{|l|p{10cm}|}
\hline
\textbf{Token Type} & \textbf{Regex Pattern} \\
\hline
COMMENT & comments \\
\hline
STRING & Pattern for string literals \\
\hline
COMPLEX & Pattern for complex numbers \\
\hline
RATIONAL & Pattern for rational numbers \\
\hline
FLOAT & Pattern for floating-point numbers \\
\hline
INTEGER & Pattern for integers \\
\hline
BOOLEAN & Patterns for true and false values \\
\hline
IDENTIFIER & Pattern for identifiers \\
\hline
PARENTHESES & Patterns for opening and closing parentheses \\
\hline
SPECIAL SYMBOLS & Patterns for quote, dot, and other special symbols \\
\hline
WHITESPACE & Pattern for whitespace characters \\
\hline
\end{tabular}
\end{table}

\subsection{Tokenization Algorithm} \label{app:tokenize}
\begin{algorithm}
\caption{Tokenization Algorithm for Scheme}
\label{alg:tokenize}
\begin{algorithmic}[1]
\Procedure{Tokenize}{input}
    \State Initialize token collection and state
    \While{not at end of input}
        \State Find next matching pattern
        \If{no match found}
            \State Report error for unexpected character
        \Else
            \State Create token from matched text
            \If{token is not whitespace or comment}
                \State Add to token collection
            \EndIf
            \State Update position in input
        \EndIf
    \EndWhile
    \State Add end-of-file token
    \State \Return token collection
\EndProcedure
\end{algorithmic}
\end{algorithm}
\subsection{Trampolining Algorithm} \label{app:trampoline}

\begin{algorithm}
\caption{Trampolining Algorithm for Tail Call Optimization}
\label{alg:trampoline}
\begin{algorithmic}[1]
\Procedure{ExecuteProcedure}{state, procedure, arguments}
\State currentProc $\gets$ procedure
\State currentArgs $\gets$ arguments
\While{true}
    \While{currentProc is TailCall}
        \State currentProc $\gets$ tailCall.proc
        \State currentArgs $\gets$ tailCall.args
    \EndWhile
    
    \State result $\gets$ currentProc(state, currentArgs)
    
    \If{result is not TailCall}
        \State \Return result
    \Else
        \State currentProc $\gets$ result
    \EndIf
\EndWhile
\EndProcedure
\Procedure{InterpretTailCall}{state, expression}
\If{expression is a procedure call}
\State Create TailCall object
\State \Return ExecuteProcedure with TailCall
\Else
\State \Return normal interpretation
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Recursive Descent Parser for Scheme}
\label{alg:recursive-descent}
\begin{algorithmic}[1]
\Procedure{Parse}{tokens}
    \State \Return \Call{ParseExpression}{}
\EndProcedure

\Procedure{ParseExpression}{}
    \State token $\gets$ current token
    
    \If{token is LEFT\_PAREN}
        \State Consume LEFT\_PAREN
        \State expressions $\gets$ Parse all expressions until RIGHT\_PAREN
        \State Consume RIGHT\_PAREN
        
        \If{expressions is empty}
            \State \Return new EmptyList()
        \ElsIf{first expression is "define"}
            \If{second expression is identifier}
                \State \Return new Definition(name, value)
            \Else
                \State \Return new FunctionDefinition(name, params, body)
            \EndIf
        \ElsIf{first expression is "let"}
            \State \Return new LetExpression(bindings, body)
        \Else
            \State \Return new FunctionApplication(operator, arguments)
        \EndIf
        
    \ElsIf{token is QUOTE}
        \State Advance token
        \State \Return new QuoteExpression(\Call{ParseExpression}{})
    \ElsIf{token is literal}
        \State Advance token
        \State \Return new LiteralExpression(token.value)
    \ElsIf{token is IDENTIFIER}
        \State Advance token
        \State \Return new VariableExpression(token.lexeme)
    \EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}
\end{document}

