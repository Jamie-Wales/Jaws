%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%
%  FINAL REPORT - Jaws: Jaws Awesomely Wrangles Scheme 
% 
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\documentclass[final]{cmpreport_02}

\usepackage{rotating}
\usepackage{listings}
\lstset{
  basicstyle=\ttfamily,
  keywordstyle=\color{blue},
  commentstyle=\color{green!60!black},
  stringstyle=\color{red},
  showstringspaces=false,
  numbers=left,
  numberstyle=\tiny,
  numbersep=5pt,
  frame=single,
  breaklines=true
}
\usepackage{subfloat}
\usepackage{color}
\usepackage{pdfpages}
\usepackage{algorithm}
\usepackage{algorithmicx}
\usepackage{algpseudocode}

\title{Jaws: Jaws Awesomely Wrangles Scheme}
\author{Jamie Michael Wales}
\registration{100067069}
\supervisor{Dr. Rudy Lapeer}
\ccode{CMP-6013Y}

\summary{
The abstract of your report summarises your entire work (and as such, your report) in no more than half a page. It should include the context of your work including its main objective, what methods you employed, how you implemented these, what the outcomes were and a final statement as a conclusion. It should not contain acronyms, abbreviations, elements of a literature review (though a statement of related work is permissible if it is crucial to your work) or future work. The abstract should be written when everything else has been written up and the project is finished!
}

\acknowledgements{
Thank you to Dr. Rudy Lapeer, who helped channel my enthusiasum into a well rounded academic project. Thank you to Ella my partner who is the glue that holds my life together. To my brother, who will always listen to me go on a tangent about compilers and to my parents for their love and support.
}

\begin{document}

\section{Introduction}

Compiler construction presents a unique challenge in computer science, requiring both theoretical depth and practical engineering skill. As Cooper and Torczon \cite{cooper2011engineering} observe, this blend of theory and practice is unparalleled in the discipline. This project explores this duality through the lens of Scheme, a functional programming language that embodies elegant mathematical principles while demanding robust implementation techniques.

The academic landscape of compiler development is often perceived as forbidding terrain. Ghuloum captures this sentiment perfectly: ``The novice compiler writer stands puzzled facing an impenetrable barrier, 'better write an interpreter instead'.'' \cite{ghuloum2006incremental}. Consequently, many avoid direct compilation to machine code, favouring interpreters or bytecode virtual machines â€“ an approach popularised by Java \cite{oracle2024java} and adopted by numerous Scheme implementations \cite{SchemeImplementations}.

This project deliberately challenges that perception by charting a direct course from abstract mathematics to concrete machine code for the Scheme language. Beginning with the theoretical underpinnings in Church's lambda calculus \cite{church1936unsolvable} and McCarthy's foundational work on Lisp \cite{mccarthy1960recursive}, we trace how these mathematical abstractions evolve into practical implementation concerns: How does one represent closures efficiently in memory? How can tail recursion be guaranteed to execute in constant space? How might functional immutability be translated effectively onto CPU architectures designed around mutation?

The dissertation follows this journey through the development of a complete Scheme implementation conforming to the R7RS specification \cite{r7rs}. We begin by grounding the work in mathematical foundations of lambda calculus, recursive function theory, and formal semantics. These principles inform the design of an initial interpreter that captures Scheme's semantics. This interpreter then serves as a stepping stone towards a native code compiler, evolving through a series of intermediate representations (IR's). Key transformations involve converting Scheme code into A-Normal Form (ANF) \cite{flanagan1993essence} to make evaluation order explicit, followed by further stages leading to the QBE Intermediate Language \cite{qbe_il}. QBE provides a suitable abstraction layer for generating efficient x86-64 machine code, chosen for its balance between low-level control and cross-platform potential. Foundational parsing techniques draw inspiration from established compiler literature \cite{aho2006compilers}, adapted for Scheme's unique homoiconic structure.

This roadmap aims not only to demonstrate \emph{how} to implement a functional language but also to reveal the fascinating interplay between abstract theory and concrete engineering. By following this path from mathematics through interpretation and compilation to machine code, we seek to demystify the compiler construction process and show how theoretical concepts are used in a practical engineering context.

The remainder of this dissertation is organised as follows: Section 2 provides the necessary theoretical background on functional programming principles and the history and significance of Lisp and Scheme. Section 3 details our methodological approach to constructing both the interpreter and the compiler. Section 4 discusses the specific implementation details of the "Jaws" system and presents evaluation results. Finally, Section 5 concludes with reflections on the project's outcomes and directions for future work.

\section{Background}

Building a compiler requires integrating deep theoretical knowledge with practical engineering skills \cite{cooper2011engineering}. This project delves into both aspects by exploring the implementation of Scheme, a language whose elegance stems from strong theoretical roots but whose practical realisation presents interesting engineering challenges. To fully appreciate Scheme as a compilation target and understand the design choices in this work, it is essential to examine two key areas: the fundamental principles of functional programming and the historical context provided by Lisp and its descendant, Scheme.

\subsection{Functional Programming Foundations}

Functional programming is a programming paradigm that models computation on mathematical foundations, specifically lambda calculus as formalised by Church \cite{church1936unsolvable}. It uses lambda calculus as the basis for its computation model, where a program can be considered based on its inputs and outputs. Computation proceeds through a series of steps, where an input is propagated through a program, contrasting with traditional approaches that rely on mutable state and object-oriented abstraction.

As Hickey, the developer of the popular functional programming language Clojure, argues in \cite{hickey2009there}, object-oriented approaches to separation of concerns is "still a mess, it's just that particular objects mess." This highlights how functional programming provides mathematical foundations and immutability at its core, making it easier to reason about programs.

Key principles of modern functional programming include:
\begin{itemize}
    \item Pure functions that produce the same output for the same input without side effects
    \item Immutable data structures that cannot be changed after creation
    \item First-class functions that can be passed as arguments or returned as values
    \item Higher-order functions that operate on other functions
\end{itemize}

Languages like Haskell enforce these principles strictly, while others like Scheme and ML provide them as preferred tools while allowing imperative features when needed. These principles enable powerful abstractions for handling complexity. Pattern matching, algebraic data types, and monadic composition allow programmers to express complex operations clearly and compose them reliably. The emphasis on immutability and pure functions makes concurrent programming more manageable, as shared state - a major source of bugs in concurrent systems, is avoided by design.

While these abstractions provide elegant ways to express computation, implementing them efficiently on real hardware presents significant challenges. Modern CPUs are designed around mutable state and sequential operations, making the compilation of functional concepts like immutability and higher-order functions non-trivial. The gap between mathematical elegance and practical realisation has existed since the early days of functional programming, where pioneers had to bridge lambda calculus's abstract notions with the realities of computer architecture. These implementation challenges, from closure representation to efficient recursion, remain relevant for modern compiler writers, making the history of how early languages like Lisp solved them particularly instructive. These factors make functional language implementation an interesting problem space for this project.

\subsection{Lisp and Scheme}

Church's work provides the proper context within functional programming, as it forms the DNA of these programming paradigms. However, it does not translate practically; it has no basis in actual computing. While lambda calculus provides the theoretical foundation, Lisp (List Processing) represents the first major practical realisation.

McCarthy used Church's work to define a language to abstract computation in his seminal paper "Recursive Functions of Symbolic Expressions and Their Computation by Machine, Part I" \cite{mccarthy1960recursive}. It successfully translated the mathematical concepts of Church into a concrete implementation, and its influence can be seen in the DNA of any modern functional programming language.

McCarthy introduced S-expressions as a way to represent both data and programs, demonstrating how complex data can be built from pairs. This approach supports atomic symbols and constructed expressions through list structures. The paper explains the \texttt{eval} function which serves as an interpreter; along with the homoiconic structure of Lisp, this provides the beginnings of Lisp's metacircular evaluation and metaprogramming capabilities.

The paper establishes an approach to defining recursive functions, explaining how control flow and base cases can ensure termination. This approach remains clear today in recursive algorithm design. The paper also outlines approaches to memory management that are still relevant: free storage lists for dynamic memory allocation, as well as garbage collection for automatic memory management. It explains a linked list structure for efficient memory use, an approach many functional languages still use today for list implementation.

Lisp's influence extends far beyond functional programming languages. Its fundamental concepts have become part of modern programming practice:
\begin{itemize}
    \item Garbage collection, first introduced by McCarthy, is now standard in languages from Java to Python.
    \item Lisp's treatment of functions as first-class values appears in JavaScript's callbacks, Python's decorators, and Rust's closures.
    \item The concept of code-as-data revolutionised how we think about metaprogramming - modern build tools, domain-specific languages, and code generation all echo Lisp's homoiconic approach where programs can manipulate other programs.
\end{itemize}

However, McCarthy's paper has some limitations. While it establishes fundamental concepts, it is limited in its usefulness when looking for applied approaches. It is steeped in heavy mathematical formalism, unsurprising given McCarthy's background as a mathematician. This emphasis on lambda calculus and recursive function theory can obscure practical implementation strategies.

Modern compilers need to address a wide range of problems not defined within this paper, unsurprising given its age. The paper's age also affects its immediate applicability. It explains implementation details for the IBM 704, which has little relevance to modern CPU architectures. McCarthy's memory concerns are a product of his time; we face different performance considerations and memory constraints than those of the 1960s.

Nevertheless, McCarthy's work remains seminal and extremely relevant to this project. It provides methodologies and formal reasoning for language concepts and well-defined procedures such as \texttt{car}, \texttt{cdr}, and others. These operations, while named for IBM 704 assembly instructions (Contents of Address Register and Contents of Decrement Register), established patterns for list manipulation that persist in modern functional languages - Haskell's \texttt{head} and \texttt{tail}, ML's pattern matching, and even Rust's iterators all echo these fundamental list operations.

The Scheme Programming Language itself, born out of the MIT Artificial Intelligence Laboratory \cite{SchemeInterpMIT}, is a dialect of Lisp \cite{structureandinterp}. As one of the foundational functional programming languages, it refined many of McCarthy's ideas while adding powerful features and emphasising lexical scoping and first-class procedures. Its minimalist design philosophy allows for a small core language, yet it remains highly expressive, partly due to its powerful macro system. Our implementation follows the R7RS specification \cite{r7rs}, ensuring compatibility with the contemporary standard of the broader Scheme ecosystem.

McCarthy's work directly shapes our implementation approach. His S-expressions guide our parser design, while his \texttt{eval} function models our interpreter. Like McCarthy, we build complex features from simple primitives, starting with basic arithmetic and gradually adding closures and tail calls. This keeps our implementation both correct and manageable.

\subsection{Compiler Construction Approaches}

While direct compilation to native machine code offers potentially the highest performance, alternative implementation strategies are common, particularly in educational settings or for languages with highly dynamic features. One prevalent approach involves compiling the source language to an intermediate bytecode, which is then executed by a virtual machine (VM). This strategy was famously successful for Java \cite{oracle2024java} and is also employed by several Scheme implementations \cite{SchemeImplementations}. Bytecode VMs can simplify the compilation process by targeting a stable, abstract machine instruction set and can aid portability, but often introduce a layer of interpretation overhead compared to native code.

This project, however, pursues the more direct path of native code compilation via an intermediate representation (QBE IL) to target x86-64, aiming to explore the challenges and potential benefits of generating efficient machine code directly. Successfully navigating this path requires drawing upon foundational theoretical works (like McCarthy \cite{mccarthy1960recursive} and SICP \cite{structureandinterp}), core specifications (like R7RS \cite{r7rs}), and leveraging practical implementation guides that bridge abstract requirements with concrete coding patterns. Resources such as Wilson's unpublished manuscript \cite{wilson1996introduction} and Maxwell's course notes \cite{intro_scheme}, both titled "An Introduction to Scheme and its Implementation," serve this purpose. While potentially dated or incomplete in some areas (particularly Wilson's older work relative to current R7RS revisions), these types of resources offer valuable visual blueprints and code examples for complex aspects like memory layout, object representation, runtime system structure, and the implementation of features like macros and first-class procedures. They provide proven patterns that help translate the formal specifications and theoretical concepts into a working system, complementing lower-level guides focused purely on code generation.


\section{Methodology}

The way in which this project was approached was in three seperate sections. First, develop a interpreter for the Scheme Programming Language. Second, develop a compiler for the Scheme Programing Language. Third, develop a web-based interface for the Scheme Programming Language.
From the outset it was clear that there was a huge amount of work to be done in order to complete the project. To give the project the best chance of developing every part a standard tree-walk interpreter approach was selected for the interpreter.
A tree-walk interpreter is the simplest of the approaches to interpretation where we walk an abstract syntax tree evaluating it as we process it. This approach was selected as it is the simplest to implement and would allow for the development of the compiler and web-based interface to be developed thereafter.
It is important to note that a compiler, would use the same front-end as an interpreter but split it's pipeline off. The compiler would generate some sort of intermediate representation (IR) which would then be converted into machine code.

\subsection{Interpreter}

\subsubsection{Lexer}
The initial development of any compiler/interpreter project begins with establishing an effective front-end strategy. This critical first phase involves selecting appropriate approaches for lexing and parsing to transform raw text into a structured format.
For lexical analysis, several methodologies were considered, including, manual character-by-character scanning with explicit state management, using lexer generator tools like Flex or ANTL. Or finally, Regular expression-based pattern matching
The regular expression approach was selected for several key reasons. 
First, regular expressions naturally map to the deterministic finite automaton (DFA) model described by Cooper and Torczon \cite{cooper2011engineering} for lexical analysis. Second, they offer a more concise and maintainable way to express token patterns compared to manual character processing. Third, they avoid the additional learning curve and external dependencies associated with lexer generators, keeping the project self-contained.
This methodology involves organising token patterns in a prioritised list, with more specific patterns (like keywords) checked before more general ones. This prioritisation is crucial for correct tokenization since patterns may overlap. For instance, scheme-specific patterns for complex numbers needs to be checked before simpler numeric patterns to ensure proper recognition.
For keyword handling, a two-stage approach was selected: first identify all text as a general identifier, then check a keyword map to determine if it's actually a reserved word. This approach simplifies the regex patterns while maintaining the language's semantics.
The complete set of token patterns and their priorities were determined by analysing the R7RS Scheme specification \cite{r7rs}, ensuring comprehensive coverage of the language syntax.

\subsubsection{Parser}
Parsing naturally follows lexing in the construction of an interpreter. The academic literature on parsing strategies is extensive, \cite{aho2006compilers} dedicating significant coverage to the topic. 
As with lexing, there exists tools that generate parsers automatically, typically producing table-based implementations that would be prohibitively complex to write manually. For this project, a recursive descent parsing (Algorithm) approach was selected, leveraging Scheme's homoiconic nature.
The extensive amount of parsing strategies in compiler theory largely stems from the complexity of handling operator precedence. Standard programming languages employ various operator types: postfix (after the operands), prefix (before the operands), infix (between operands), and even mixfix (such as the ternary operator, which interleaves with its operands). Different parsing techniques attempt to efficiently distinguish between these forms.
However, much of the literature on parsing optimisation originates from an era when computational efficiency was more constrained. Modern hardware has somewhat diminished these concerns relative to implementation simplicity and maintainability.
Furthermore, Scheme's homoiconic syntax, where every operation is uniformly expressed with parentheses in prefix notation, means there is no need for precedence based parsing significantly simplifies the parsing challenge. Ultimately, it eliminates the need for complex precedence handling, making recursive descent parsing a straightforward and appropriate methodology for this project.

\subsubsection{Macros}
While most programming languages proceed directly from parsing to interpretation, Scheme introduces a crucial intermediate step: macro expansion. Scheme's powerful macro system represents one of its most distinctive features, enabling sophisticated syntactic extensions that transform the Abstract Syntax Tree before evaluation begins.

Unlike traditional C-style preprocessor macros that operate through simple text substitution, Scheme implements a hygienic macro system that works with structured expressions. This methodological approach provides significant advantages:

\begin{itemize}
    \item Elimination of variable capture problems and unintended evaluation order issues
    \item Pattern-matching capabilities that recognize and transform complex syntax patterns
    \item Preservation of lexical scoping during expansion
    \item The ability to create domain-specific language extensions within Scheme itself
\end{itemize}

The implementation for this project follows the R7RS specification's syntax-rules system, which defines macros as pairs of patterns and templates. When a macro is invoked, the system matches the invocation against defined patterns sequentially until finding a match, then generates the corresponding template with appropriate substitutions.

For example, the following macro definition creates an intuitive 'for' loop syntax that expands to a map operation:

\begin{figure}[h!]
\caption{Example Scheme Macro Definition}
\begin{lstlisting}[language=Lisp]
(define-syntax for
  (syntax-rules (in as)
    ((for element in list body ...)
     (map (lambda (element)
            body ...)
          list))
    ((for list as element body ...)
     (for element in list body ...))))
\end{lstlisting}
\end{figure}

This macro enables more readable code like this:

\begin{figure}[h!]
\caption{Example Macro Usage}
\begin{lstlisting}[language=Lisp]
(for x in '(1 2 3 4)
     (display x)
     (* x x))

(* This expands to: *)
(map (lambda (x)
       (display x)
       (* x x))
     '(1 2 3 4))

\end{lstlisting}
\end{figure}

To ensure proper hygiene, the implementation required a separate evaluation environment for macro expansion. This methodology involves tracking the origin of each identifier and performing appropriate renaming during expansion, preventing variables in the macro definition from capturing variables at the macro use site.
The macro expansion process transforms the initial AST into an expanded form where all macros have been replaced with their expansions. This approach aligns with the classic Scheme philosophy of providing a minimal core language with powerful syntactic extensions built on top, effectively creating an extended language that is expressed in terms of simpler core constructs.

\subsubsection{Evaluation}
Once an AST has been developed and processed through macro expansion, the evaluation phase begins. This phase is where the Scheme language's semantics are applied to execute the program. The R7RS Scheme specification defines several key requirements that any compliant implementation must satisfy.

Scheme's evaluation model requires lexical scoping, where variables are resolved in the environment where they were defined, not where they are used. This contrasts with dynamic scoping found in some other languages and necessitates a hierarchical environment structure. Additionally, Scheme treats procedures as first-class values that can be passed as arguments, returned from functions, and stored in data structures, requiring the evaluation system to handle functions as ordinary values.

The language specification also defines special forms such as \texttt{if}, \texttt{lambda}, and \texttt{define} that follow unique evaluation rules different from regular function calls.To clarify why define must be a special form in Scheme: If define were implemented as a regular procedure call, the interpreter would evaluate its arguments before applying the procedure, following standard applicative-order evaluation. This would cause the interpreter to evaluate the variable name before it's defined, leading to errors or incorrect bindings. Additionally, the environment in which definitions occur is crucial - define must create bindings in the current lexical environment rather than in the evaluation environment of a procedure call. These requirements necessitate that define be implemented as a special form with its own unique evaluation rules that differ from regular procedure calls, allowing it to properly manipulate the environment structure according to Scheme's lexical scoping rules. For instance, conditional expressions only evaluate the necessary branches, and lambda expressions create closures without evaluating their bodies. Perhaps most importantly, the R7RS specification mandates tail call optimisation, requiring implementations to optimise tail calls to prevent stack overflow in recursive procedures. This optimisation allows recursive procedures to execute in constant space when the recursion occurs in tail position.

The evaluation process uses a dispatch mechanism that selects the appropriate evaluation strategy based on the node type. Self-evaluating forms like numbers and strings simply return their value. Variable references are resolved by searching the environment chain. Special forms are handled by dedicated evaluation functions that implement their specific rules. For example, the \texttt{lambda} evaluator creates a closure object containing both the function definition and a reference to the current environment, capturing the lexical context.

The requirement for tail call optimisation presented a significant challenge, particularly when implementing in a host language that doesn't natively support this feature. To address this, a technique known as "trampolining" \ref{alg:trampoline}was selected. When a procedure call is identified as being in tail position (the final expression to be evaluated in a function body or conditional branch), instead of executing the call directly, the evaluator returns a tailcall object containing the procedure and arguments. The main evaluation loop detects these tailcall objects and executes them iteratively without growing the call stack. This effectively transforms recursive processes into iterative ones at the implementation level.

\subsubsection{Values}
During interpretation objects must be created to represent the values that are being manipulated. These objects need to represent the many different types as per the RSR7 specification. These objects are as follows:
\begin{itemize}
    \item \textbf{Number}: Scheme's numeric tower includes integers, rationals, and floating-point numbers. The interpreter must support all these types and provide appropriate arithmetic operations for each.
    \item \textbf{String}: Scheme strings are sequences of characters enclosed in double quotes. The interpreter must provide operations for creating, accessing, and manipulating strings.
    \item \textbf{Character}: Scheme has a distinct character type for representing individual characters. The interpreter must provide operations for creating, comparing, and manipulating characters.
    \item \textbf{Symbol}: Symbols are unique identifiers used to represent variables, functions, and other named entities in Scheme. The interpreter must provide operations for creating, comparing, and interning symbols.
    \item \textbf{Boolean}: Boolean values in Scheme are \texttt{\#t} for true and \texttt{\#f} for false. The interpreter must provide operations for creating, comparing, and manipulating boolean values.
    \item \textbf{Pair}: The fundamental data structure in Scheme, pairs are used to construct lists and other compound data structures. The interpreter must provide operations for creating, accessing, and manipulating pairs.
    \item \textbf{Vector} Vectors are fixed-length sequences of elements that can contain any type of value. The interpreter must provide operations for creating, accessing, and manipulating vectors. This includes bytevectors which are vectors of bytes.
    \item \textbf{Procedure} Procedures in Scheme are first-class values, the interpreter needs to treat these as such. Procedures need to store their own environment of symbboles and values. As well as constructing closures, which are procedures that return procedures, that have access to their parents scope.
    \item \textbf{Port}: Ports are Scheme's abstraction around input and output. The interpreter must provide operations for creating, reading, and writing to ports.
    \item \textbf{Continuations}: Continuations are a powerful feature of Scheme that allow for the capture and reification of the current state of the program. The interpreter must provide operations for creating, capturing, and invoking continuations.
\end{itemize}

Scheme is dynamically typed, which means an objects type is deterimined at runtime. Therefore the interpreter must be able to handle the many different types of objects that can be created. 
Approaches on how to handle this within the host language differ (ADD REF). There are three main approaches, these are bloated structs, tagged unions/polymorphic variants, tagged pointers. 
Bloated structs refer to a structure that holds all the possible values that an object can hold. This is the most memory intensive approach but is the fastest. Tagged unions are a more memory efficient approach but are slower than bloated structs. Tagged pointers are the most memory efficient approach but are the slowest.
During implementation polymorphic variants were selected as the approach to handle the many different types of objects that can be created, whilst maintaining type safety and satisfying the host langauges type system. 
This approach was selected as it is middle ground between tagged pointers and bloated structs, and makes imeplementation easier and more maintainable.

\subsection{Compiler}
A major part of this project was working toward a compiler for the Scheme programming language. Modern are complex programs in their own right. In order to understand the complexity of these programs, two intermediate steps we're delivered.
First was transforming the AST into an intermediate representation (IR). This IR is a data structure that represents the program in a way that is easier to manipulate and transform. The second was to perform to perform performance optimisations on these representations.
The intermediate representation (IR) is the core data structure in a compiler, serving as the central model of the program being compiled. Most compiler components interact with this IR, reading from it and modifying it. Therefore, the design choices about what information to include in the IR and how to structure it significantly impact both compilation efficiency and the quality of the resulting code. \cite{cooper2011engineering}
The intermediate representations chosen for the project we're A Normal Form (ANF) and Three Address Code (TAC). For ANF the optimisation chosen is dead code elimination and constant folding. For TAC the optimisation chosen was scope hoisting and inline definition.

\subsubsection{A Normal Form}
Administrative Normal Form \cite{flanagan1993essence} is a compiler intermediate representation where every non-trivial expression is assigned to a variable, ensuring that function arguments are always simple atomic values (like variables or constants) rather than complex expressions. This transformation makes the evaluation order explicit and simplifies subsequent compiler optimisations by breaking complex nested expressions into a sequence of simpler statements, similar to how statements work in imperative languages but preserving functional semantics.
There are two compelling reasons ANF was selected as the first intermediate representation in many functional language compilers. First, ANF naturally corresponds to Scheme's fundamental structure, as Scheme programs can be expressed as nested let-bindings, making ANF transformations particularly elegant. Second, ANF serves as an ideal bridge between high-level functional code and lower-level representations, facilitating subsequent optimisation passes by exposing the program's evaluation order while maintaining its pure functional semantics.

\subsubsection{Dead Code Elimination}
Dead code elimination (DCE) integrates seamlessly with ANF due to the clear execution flow that ANF provides. Each variable binding in ANF creates an explicit dependency relationship, making it straightforward to construct a dependency graph where nodes represent variables and edges represent usage relationships. This directed graph can then be systematically traversed to identify nodes with no dependents, indicating unused computations. By removing these unreachable nodes from the graph and their corresponding code from the program, we eliminate dead code without altering program semantics. This approach is particularly effective because ANF's explicit naming of intermediate values makes dependencies immediately visible, enabling more precise identification of truly dead code than would be possible with nested expressions. 
(As shown in Appendix \ref{alg:dce})

\subsubsection{Constant Folding}
Constant folding represents the second optimisation technique implemented alongside dead code elimination. This transformation becomes particularly elegant within the ANF representation, where all expressions are explicitly bound to variables. The structure of ANF guarantees that every node in the program representation is either a function application or an atomic value.
By leveraging this, the compiler can systematically traverse the ANF structure to identify expressions whose operands are all compile-time constants. These expressions can then be evaluated during compilation rather than at runtime, replacing the original expression with its computed result. For example, expressions like \texttt{(+ 3 4)} can be reduced to the constant \texttt{7}.
This optimisation produces benefits such as reducing the instruction count in the generated code, eliminating unnecessary computations. This optimisation was chosen as it can be implemented with basic functionality initially and extended to handle increasingly complex constant expressions as needed.
Unlike dead code elimination, which removes unused code paths, constant folding transforms the code while preserving its semantic meaning. The combination of these two optimisaitons working on the ANF representation demonstrates the power of well-chosen intermediate representations in enabling effective compiler transformations.
(As shown in Appendix \ref{alg:const-fold})


\subsubsection{Three Address Code}
Three Address Code (TAC) is a lower-level intermediate representation where each instruction typically involves at most three addresses (two operands and one result), closely mirroring the structure of assembly language instructions. In this representation, complex expressions are decomposed into a sequence of simpler operations, with each operation producing a result that is stored in a temporary variable. This linear format eliminates nested expressions entirely, making the control flow and data dependencies explicit.
TAC was selected as the second intermediate representation in the compilation pipeline for several strategic reasons. First, it provides a natural progression from ANF, further flattening the program structure while preserving the essential computational logic. Second, TAC's direct correspondence to QBE IR facilitates straightforward code generation, as each TAC instruction can typically be translated into one or a small number of QBE instructions. Finally, TAC enables a range of powerful optimisations that operate effectively on this representation.
The optimisations implemented at the TAC level include scope hoisting, which elevates variable definitions to their optimal scope level, reducing unnecessary allocations and improving memory access patterns. Additionally, inline definitions allow for replacing variable references with their values directly where beneficial, reducing indirection and potentially enabling further optimisations. These transformations are particularly well-suited to the TAC representation due to its explicit control flow, which makes dependency analysis more straightforward and precise. Unlike ANF optimisations which focus on expression simplification, TAC optimisations leverage the linear instruction sequence to identify broader structural improvements that align well with QBE IR's execution model.

\subsubsection{QBE Code Generation}
After experimentation with direct assembly code generation, QBE Intermediate Language \cite{qbe_il} was selected as the final compilation target for this project. QBE (Quick Backend) provides an ideal balance between low-level control and cross-platform compatibility, significantly accelerating the development process while maintaining fine-grained control over the generated code.
QBE offers several strategic advantages for this compiler implementation. First, its streamlined instruction set closely resembles assembly while abstracting away architecture-specific details, enabling the generation of efficient code for multiple target platforms without maintaining separate codebases. Second, QBE's simplified type system which comprises the essential types w (word), l (long), s (single), and d (double) this maps to Scheme's runtime representation needs without introducing unnecessary complexity.
The translation from TAC to QBE is direct, as both share a similar three-address code structure. Most TAC operations map directly to QBE instructions, with minimal adaptation required. Memory operations map cleanly to QBE's load and store instructions, which handle different data widths explicitly.
Control flow translation is equally straightforward, with QBE's block-based structure accommodating the program's flow graph representation. Conditional branches in TAC translate to QBE's jnz instruction, while function calls align with QBE's call mechanism. This direct correspondence significantly simplified the code generation phase of the compiler, allowing development efforts to focus on optimisation rather than architecture-specific details.
By leveraging QBE, the compiler achieves a balance of performance, portability, and development efficiency that would be difficult to accomplish with direct assembly generation. The resulting compilation pipeline delivers robust, optimised code while maintaining the flexibility to target multiple architectures with minimal additional effort.

\begin{itemize}
\item \textbf{Methodology: } Explanation (and justification) of methods, algorithms (typically written in pseudo-code), mathematical or statistical models, technologies etc.\ that you will implement as part of your project. These may come from other sources (e.g.\ the literature, Github, etc.) or be your own creation. Note that it should not cover methods that you will not use! If these are worthwhile mentioning then briefly discuss them in the Background section instead.
\item \textbf{Design: } Design of experiments, design of a survey or design of a system that consists of multiple components e.g. software (use preliminary diagrams to describe the design) or a physical manifestation such as an embedded system, a robot, etc.
\item \textbf{Plan(ning): } Gathering of data, description of experiments (experimental plan), testing and evaluation planning. Experiments could be in the fields of data science, machine learning, signal processing, graphics, etc. Evaluation metrics could include performance speed, accuracy, relevance, etc. Although evaluation is part of the next section, evaluation \textbf{metrics} should be explained here.
\end{itemize}

\section{Implementation}
This section details the pratical development of Jaws. It outlines the actual development based on the above methodology. The implementation has the same three part structure as the methodology. A RSR7 specifciation compliant Scheme interpreter, a compiler that generates efficient machine code via the QBE intermediate representation, and a web-based interface for practical usability.

\subsection{Architecture Overview}
The implementation follows a functional pipeline architecture, where each component operates independently and transforms its input into a representation suitable for the next stage:

\texttt{Input Source -> Lexer -> Parser -> Macro Expander ->
    -> Interpreter
    -> Compiler [ANF -> TAC -> QBE] -> Machine Code + Runtime } 

This pipeline design offers several key advantages:
\begin{itemize}
\item \textbf{Component Independence}: Each stage operates as a pure function, transforming data without shared state. This isolation simplifies testing and modification of individual components.
\item \textbf{Extensibility}: New transformations or optimisations can be inserted at any pipeline stage without affecting other components.
\item \textbf{Functional Purity}: The stateless design aligns naturally with Scheme's functional paradigm, particularly beneficial for implementing features like the reader and evaluator procedures.
\end{itemize}
The implementation is structured into three main subsystems:
\subsubsection{Core Language Implementation}
The interpreter's foundation consists of built-in procedures implemented in the host language, organized into specialized namespaces:
\begin{itemize}
\item \texttt{jaws\_math}: Mathematical operations and numeric type handling
\item \texttt{jaws\_eq}: Equality and type predicates
\item \texttt{jaws\_io}: Input/output operations
\item \texttt{TODO}
\end{itemize}
These core procedures provide the essential functionality required by the R7RS specification, serving as building blocks for more complex features.
\subsubsection{Standard Library}
Following R7RS conventions, much of the language's functionality is implemented in Scheme itself, located in the \texttt{lib/} directory. This approach:
\begin{itemize}
\item Demonstrates the language's expressiveness
\item Provides a clear separation between core and derived features
\item Enables easy modification and extension of standard procedures
\end{itemize}
The import system allows these library procedures to be dynamically loaded, supporting modular program construction and code reuse.
\subsubsection{Runtime System}
The compiler's output interfaces with a runtime system implemented in C, located in the \texttt{runtime/} directory. This runtime:
\begin{itemize}
\item Provides low-level memory management and garbage collection
\item Implements primitive operations required by compiled code
\item Bridges between compiled Scheme code and the operating system
\end{itemize}
The runtime is compiled as a dynamic library and linked with the generated machine code, providing essential services while maintaining flexibility in implementation and optimisation.
This architectural approach, combining functional purity with clear component boundaries, proved particularly effective for implementing Scheme's dynamic features while maintaining code quality and testability. The following sections detail the implementation of each major component, beginning with the lexical analyzer.


\subsection{Lexing}
The lexer, is the first stage in the processing pipeline, responsible for converting the raw input character stream into a sequence of tokens. Following the methodology, a regular expression-based approach was implemented using C++'s standard \texttt{<regex>} library.

A ScanState struct encapsulates the lexer's state, including the input string, the current character position the current line number,column number and a vector of strings storing the content of each line. 

This state object is passed through the scanning functions which return updated state information along with results.This functional style of passing data through functions, rather than relying on classes and mutable state, makes the lexer's core logic self-reliant. Crucially, this design makes the tokenize function reusable, which is essential for implementing dynamic Scheme features like the read procedure, as read needs to invoke the lexer on demand at runtime to process data from input ports.

Tokens represent the syntax of Scheme, patterns are defined in the regexPatterns vector, a list of RegexInfo objects. Each RegexInfo pairs a std::regex with a corresponding Tokentype enum value. The order in this vector is crucial, implementing the prioritisation strategy discussed in the methodology, more specific patterns before more general ones such as identifiers or plain integers. This insures correct matching. 

The \texttt{std::regex\_search} function with the \newline \texttt{std::regex\_constants::match\_continuous} flag is used in the matchNextToken function to find the longest match at the current position in the remaining input string. 

A two-stage approach handles keywords. Identifiers are first matched using a general regex pattern \texttt{Tokentype::IDENTIFIER}. Subsequently, the matched lexeme is looked up in the keywords map. If found, the token's type is updated to the specific keyword's Tokentype otherwise, it remains an IDENTIFIER. This simplifies the regex patterns significantly.

The core loop is the tokenize functionwhich orchestrates the process. It iterates while the current position is within the input string. In each iteration, it calls matchNextToken. If a token is successfully matched, updateScanPosition advances the ScanState past the matched lexeme, updating line and column counts appropriately. Tokens representing WHITESPACE or COMMENT are discarded. Valid tokens are added to a list. If matchNextToken fails to find any matching pattern, a ParseError exception is thrown, indicating an unexpected character. 

This implementation leverages C++'s built-in regex capabilities effectively, providing a robust and maintainable lexer consistent with the R7RS specification, while the state management ensures accurate error reporting and facilitates dynamic usage.


\subsection{Parsing}
The parser receives the flat list of tokens from the lexer and constructs an Abstract Syntax Tree (AST) representing the program's expression structure. As outlined in the methodology, a recursive descent parser was implemented, directly benefiting from Scheme's homoiconic S-expression syntax, which eliminates the need for complex operator precedence handling. Similar to the lexer, a ParserState struct manages the parser's progress. It holds the vector of tokens, the index of the current token being processed, and a panicMode boolean flag used for basic error recovery.

The parsing process centres around the parseExpression function. This function acts as the main dispatcher, determining how to parse based on the current token. If the current token is \texttt{LEFT\_PAREN}, parseExpression advances past it and then checks the following token for Scheme's special forms \cite{r7rs}. If a special form keyword is found, control is transferred to a dedicated parsing function.

If the token after \texttt{LEFT\_PAREN} is not a recognised special form keyword, it's treated as a standard procedure call, and parseSExpression is called. If the current token is the shorthand QUOTE ('), parseQuoted is called. If the current token is \texttt{HASH} followed by \texttt{LEFT\_PAREN}, parseVector is called. Otherwise, the parser expects an atomic literal or identifier, and parseAtom is called.

Each parsing function is responsible for consuming the appropriate tokens using helper functions (advance, consume, match) and constructing a corresponding AST node. These nodes are represented as \texttt{std::shared\_ptr<Expression>}, where Expression likely contains a type safe union (e.g., \texttt{std::variant}) to hold different expression types.

Crucially for Scheme's semantics, functions parsing sequences of expressions within a body identify the last expression in that sequence. This final expression is wrapped in a TailExpression AST node. This explicit marking is essential for the interpreter or compiler backend to correctly implement tail call optimisation (TCO). Marking these in the parser has the benefit of when a macro is expanded and reparsed, the tail position is marked, and the macro expansion can be correctly optimised.

The parser's error recovery system synchronises to the next valid expression after encountering syntax errors, allowing it to report multiple errors in a single pass. It does this by consuming tokens until finding a left parenthesis followed by key Scheme keywords, then continues parsing from that point; this strategy is often referred to as panic mode \cite{nystrom2021crafting}. A hadError flag tracks if any errors occurred during parsing, determining the final success or failure of the parse operation.

The recursive descent strategy, combined with helper functions for token consumption and checking, provides a direct and understandable implementation for parsing Scheme's relatively simple core syntax, while correctly identifying structures necessary for later stages like macro expansion and tail call optimisation.

\subsection{Macro Expansion} % Changed from \subsection

Scheme's hygienic macro system, based on \texttt{syntax-rules}, is a powerful feature for syntactic abstraction, allowing programmers to define new language constructs that transform code before evaluation. Jaws implements this system through a dedicated macro expansion pass that operates on the Abstract Syntax Tree (AST) after parsing and before interpretation or compilation.

\subsubsection{Macro Representation and Environment}
Macros are defined using the \texttt{define-syntax} special form, which binds an identifier to a macro transformer. In Jaws, the transformer is typically the result of evaluating a \texttt{syntax-rules} form, represented internally as a \texttt{SyntaxRulesExpression} AST node. This node contains the list of literal identifiers that should not be treated as pattern variables, and a list of rules, each consisting of a pattern and a template expression.

These macro definitions are stored in a dedicated \texttt{MacroEnvironment}, separate from the runtime environment used for variable bindings. The \texttt{MacroEnvironment} maps macro keywords (strings) to the corresponding \texttt{Expression} node representing their definition (usually the \texttt{SyntaxRulesExpression}). Like the runtime \texttt{Environment}, the \texttt{MacroEnvironment} can be hierarchical, allowing macros to be scoped, although often a single environment is shared during expansion.

\subsubsection{Expansion Process}
The top-level entry point for macro expansion is the \texttt{expandMacros} function. It takes a sequence of expressions (typically the result of parsing a file or input) and the current \texttt{MacroEnvironment}. It iterates through the expressions:
\begin{enumerate}
    \item \texttt{DefineSyntaxExpression} nodes are used to populate the \texttt{MacroEnvironment} but are then typically removed from the sequence passed to the interpreter.
    \item Other expressions are processed by the core recursive expansion function, \texttt{transformMacroRecursive}.
\end{enumerate}
The \texttt{transformMacroRecursive} function performs a fixed-point expansion. It repeatedly traverses an input expression tree until no further macro expansions can be applied. It operates as follows:
\begin{itemize}
    \item If the current node is an identifier (represented as \texttt{MacroAtom}) that is bound to a macro in the \texttt{MacroEnvironment}, it attempts to apply the macro rules (this usually only succeeds if the macro pattern is just the identifier itself).
    \item If the current node is a list (represented as \texttt{MacroList}) whose first element is an identifier bound to a macro, it attempts to match the entire list against the rules of that macro.
    \item If a macro match is found and successfully expanded (using \texttt{tryMatch} and \texttt{transformTemplate}), the original macro call expression is replaced by the expansion result. Crucially, the process then \textbf{restarts} the expansion analysis on this \textbf{new} code structure, allowing macros to expand into forms that use other macros.
    \item If the current node is not a macro call, \texttt{transformMacroRecursive} recursively calls itself on the sub-expressions (e.g., the elements of a list) to expand any macros nested within.
    \item A recursion depth limit is enforced to detect and prevent infinite macro expansion loops.
\end{itemize}
This iterative process ensures that all macro invocations are expanded until only core Scheme forms and procedure calls remain.

\paragraph{Pattern Matching}
When a potential macro invocation is identified, the \texttt{tryMatch} function is called to match the macro invocation (the input expression) against a specific rule's pattern. Both the pattern and the input are first converted into an internal \texttt{MacroExpression} representation using the \texttt{fromExpr} helper. \texttt{tryMatch} recursively compares the structure of the pattern against the input:
\begin{itemize}
    \item \textbf{Literals:} Identifiers declared as literals in the \texttt{syntax-rules} form must match the corresponding symbols exactly in the input expression.
    \item \textbf{Pattern Variables:} Identifiers in the pattern that are not literals (and not \texttt{\_} or \texttt{.}) act as pattern variables. They match any single corresponding sub-expression in the input. These matches are captured and stored in a \texttt{MatchEnv} object, which maps the variable name (string) to a \texttt{PatternMatches} structure containing a list of the \texttt{MacroExpression}s it matched. The wildcard \texttt{\_} matches any single expression but does not bind it.
    \item \textbf{Ellipsis (\texttt{...}):} The ellipsis indicates that the preceding pattern element (which must be a pattern variable or a sub-pattern containing variables) can match zero or more elements in the input expression. When a variadic pattern matches multiple input elements, the corresponding pattern variables within that pattern are bound in the \texttt{MatchEnv} to a list containing each of the matched sub-expressions. The system handles nested ellipses and ensures that variables associated with the same ellipsis match the same number of elements.
\end{itemize}
If the entire input expression successfully matches the pattern according to these rules, \texttt{tryMatch} returns true along with the populated \texttt{MatchEnv}.

\subsubsection{Template Instantiation and Hygiene}
If a pattern matches successfully, the \texttt{transformTemplate} function instantiates the corresponding template using the bindings captured in the \texttt{MatchEnv}. It recursively traverses the template structure:
\begin{itemize}
    \item Literals and structure from the template are copied directly into the output.
    \item When a pattern variable is encountered in the template, it is replaced by the corresponding matched expression(s) stored in the \texttt{MatchEnv}.
    \item \textbf{Hygiene:} Crucially, Scheme macros are hygienic, meaning they avoid unintentional variable capture. Jaws implements this using syntax contexts (\texttt{SyntaxContext}), which attach unique 'marks' (scope identifiers) to identifiers. When \texttt{transformTemplate} substitutes a matched expression (copied from the macro input) for a pattern variable, the expression retains its original syntax context. However, when an identifier that was part of the original macro template (i.e., not a pattern variable) is copied to the output, it is given a fresh context associated with the specific macro expansion (\texttt{macroContext}), often combined with the context of the original macro definition. This ensures that variables introduced by the macro cannot accidentally capture or clash with variables in the surrounding code where the macro is used, and vice-versa. The \texttt{addUsageContextRecursive} helper likely handles applying the macro expansion context to template literals.
    \item \textbf{Ellipsis (\texttt{...}):} If a template element containing pattern variables is followed by an ellipsis, \texttt{transformTemplate} triggers variadic expansion, often delegated to helpers like \texttt{expandVariadicPatternGroup} or \texttt{expandParallel}. These functions determine the number of expansions required based on the number of matches stored in the \texttt{MatchEnv} for the 'controlling' variables (those matched by the ellipsis in the pattern). They then iterate that many times. In each iteration \texttt{k}, they create a temporary environment containing the k-th match for controlling variables and the single match (if any) for non-controlling variables, and recursively call \texttt{transformTemplate} on the template sub-expression (without the ellipsis). The results from all iterations are collected and spliced into the output structure.
\end{itemize}

\subsubsection{Final Conversion}
After the iterative expansion process by \texttt{transformMacroRecursive} completes, the final resulting \texttt{MacroExpression} tree, now free of macro calls defined by \texttt{syntax-rules}, is converted back into the standard \texttt{Expression} AST using the \texttt{convertMacroResultToExpression} function. This final AST is then ready for the interpreter or compiler.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% End of Macro Expansion Section
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
\subsection{Interpreter}

The interpreter forms the executable heart of the Jaws system, bringing the parsed and expanded Scheme code to life. It operates by traversing the Abstract Syntax Tree (AST) generated by the preceding stages, applying Scheme's evaluation rules to compute the program's result. The implementation follows the tree-walk interpretation methodology described earlier.

\subsubsection{Evaluation Semantics and Special Forms}

The core of the interpreter is the \texttt{interpret} function, which recursively evaluates expressions within a given environment. This function employs a dispatch mechanism, implemented using \texttt{std::visit} on the \texttt{Expression} variant, to select the appropriate evaluation logic based on the type of AST node encountered.

Self-evaluating expressions, such as numbers, strings, booleans, and characters, are handled by \texttt{interpretAtom}. This function directly translates the token's literal value into the corresponding \texttt{SchemeValue} representation (e.g., \texttt{"hello"} becomes a \texttt{SchemeValue} holding a \texttt{std::string}, \texttt{\#t} becomes a \texttt{SchemeValue} holding \texttt{true}). For \texttt{AtomExpression} nodes representing identifiers, \texttt{interpretAtom} looks up the identifier in the \texttt{InterpreterState}'s environment. If found, the associated \texttt{SchemeValue} is returned; otherwise, an "Undefined variable" error is thrown.

The standard evaluation rule for compound expressions, represented by \texttt{sExpression} nodes, dictates applicative-order evaluation. The \texttt{interpretSExpression} function first recursively calls \texttt{interpret} on the first element of the S-expression (the operator position). It then recursively calls \texttt{interpret} on each subsequent element (the operand positions). If the operator evaluates to a procedure (\texttt{SchemeValue} holding a \texttt{std::shared\_ptr<Procedure>}), the \texttt{executeProcedure} function is invoked with the procedure and the list of evaluated operands. This standard evaluation path is facilitated by the \texttt{evaluateProcedureCall} helper function which handles the evaluation of the operator and operands before \texttt{executeProcedure} is called.

However, Scheme requires several special forms that deviate from this standard evaluation rule. These forms are essential for control flow, definition, and syntactic abstraction, and cannot be implemented as regular procedures because they require non-standard evaluation of their arguments or direct manipulation of the execution environment. Jaws handles these via dedicated interpretation functions:

\begin{itemize}
    \item \textbf{Conditionals:} \texttt{if}. The \texttt{interpretIf} function evaluates \textbf{only} the condition expression first. Based on whether the result is true (any value other than \texttt{\#f}), it then evaluates \textbf{only} the \texttt{then} branch or the optional \texttt{else} branch, ensuring that \textbf{only} the necessary code path is executed.
    \item \textbf{Definitions:} \texttt{define}, \texttt{define-syntax}. The \texttt{interpretDefine} and \texttt{interpretDefineProcedure} functions handle definitions. \texttt{interpretDefine} evaluates the value expression and then binds the resulting \texttt{SchemeValue} to the specified identifier (\texttt{HygienicSyntax}) directly within the \textbf{current} environment. \texttt{interpretDefineProcedure} constructs a \texttt{UserProcedure} object (capturing the current environment) and binds it similarly. If \texttt{define} were a regular procedure, the interpreter would attempt to evaluate the identifier \textbf{before} it was defined, leading to an error. \texttt{define-syntax} is handled similarly during macro expansion, \textbf{not} interpretation directly.
    \item \textbf{Abstractions:} \texttt{lambda}, \texttt{let}. The \texttt{interpretLambda} function creates a \texttt{UserProcedure} object. Crucially, it captures the \textbf{current} environment as the procedure's closure environment but does \textbf{not} evaluate the procedure's body expressions at this time. The body is \textbf{only} evaluated later when the procedure is actually called via \texttt{executeProcedure}. While \texttt{let} expressions introduce local bindings, they are often implemented as syntactic sugar for an immediately invoked lambda expression. The \texttt{interpretLet} function effectively constructs and calls such an implicit lambda, evaluating the binding expressions in the current environment and then executing the body within a new environment where the bindings are established. Named \texttt{let} provides a convenient syntax for local recursive procedures, also handled by \texttt{interpretLet} by binding the procedure name within the new environment before evaluating the body.
    \item \textbf{Assignment:} \texttt{set!}. The \texttt{interpretSet} function evaluates the value expression, then modifies an \textbf{existing} binding for the identifier in the environment hierarchy. It requires the variable to be already defined.
    \item \textbf{Quoting:} \texttt{quote}, \texttt{quasiquote}, \texttt{unquote}, \texttt{unquote-splicing}. \texttt{interpretQuote} prevents evaluation entirely, returning the quoted expression structure directly as data (represented by nested \texttt{SchemeValue} lists, symbols, etc.). \texttt{interpretQuasiQuote} handles quasiquotation with its associated \texttt{unquote} and \texttt{unquote-splicing} mechanisms, described in detail later.
\end{itemize}

This distinction between standard procedure calls and special forms is fundamental to Scheme's semantics. The interpreter's structure, dispatching based on the AST node type, allows it to correctly implement these differing evaluation rules.

\subsubsection{Environment, Closures, and Control Flow}

Scheme's execution model relies heavily on its environment structure for managing variable bindings and its specific control flow mechanisms, including guaranteed tail call optimisation and first-class continuations.

\paragraph{Environment and Closures}
Lexical scoping is fundamental to Scheme and is implemented in Jaws using a hierarchical environment structure, represented by the \texttt{Environment} class. Each \texttt{Environment} object holds local bindings in an unordered map mapping \texttt{HygienicSyntax} identifiers to \texttt{SchemeValue} objects, a shared pointer (\texttt{parent}) to its enclosing environment, and a mutex for thread-safe access.

When looking up a variable using the \texttt{get} method, the interpreter searches the current environment's \texttt{variables} map first. This lookup respects macro hygiene, comparing not only the identifier name but also the associated syntactic context (marks) stored within the \texttt{HygienicSyntax} object. Specific rules apply, such as allowing global unmarked bindings or identifiers without marks to match more broadly, facilitating bindings introduced by macros. If no compatible binding is found in the current environment, the search continues recursively up the chain by calling \texttt{get} on the \texttt{parent} environment, terminating only when a binding is found or the root environment (where \texttt{parent} is null) is reached.

New bindings are introduced using the \texttt{define} method, which adds or replaces an entry directly in the \textbf{current} environment's \texttt{variables} map. Assignment to existing variables uses the \texttt{set} method, which similarly searches for a hygienically compatible binding in the current environment and updates it; if none is found, it recursively attempts the \texttt{set} operation on the parent. New scopes are created using the \texttt{extend} method, which constructs a new \texttt{Environment} object whose \texttt{parent} pointer points back to the environment it was called from.

Closures are the mechanism that enables first-class procedures while respecting this lexical scope. When a \texttt{lambda} expression is evaluated by \texttt{interpretLambda}, it creates a \texttt{UserProcedure} object. Crucially, this object captures a shared pointer to the \textbf{current} environment active at the time of definition, storing it in its \texttt{closure} member. This captured environment is used when the procedure is later invoked by \texttt{UserProcedure::executeBody}. The \texttt{executeBody} method creates the new execution environment by calling \texttt{extend} on the captured \texttt{closure} environment, ensuring the procedure body executes with access to the variables that were in scope when and where it was defined.

\paragraph{Tail Call Optimization (TCO)}
The R7RS specification mandates that Scheme implementations perform tail call optimisation, allowing infinite tail recursion to execute in constant stack space. Jaws implements this using a technique known as trampolining, managed cooperatively between the parser, the interpreter functions, and the main procedure execution logic.

As discussed previously, the parser identifies expressions occurring in tail position and wraps them in a \texttt{TailExpression} AST node. The interpreter function \texttt{interpretTailCall} is responsible for handling these nodes. If the expression wrapped by \texttt{TailExpression} is an \texttt{sExpression} that evaluates to a \texttt{UserProcedure} call, TCO is possible.

Instead of making a direct recursive call to \texttt{interpret} or \texttt{executeProcedure}, \texttt{interpretTailCall} sets specific flags and stores the target procedure and arguments within the \texttt{InterpreterState} object. It then returns \texttt{std::nullopt}, signaling to its caller that a tail call has been set up but not executed.

The actual optimisation occurs within the \texttt{executeProcedure} function. This function operates in a loop. At the beginning of each iteration, it checks if a tail call was requested by the previous step. If true, it loads the pending procedure and arguments into its local variables, resets the tail call state, and \textbf{continues to the next iteration of the loop}. This avoids creating a new C++ stack frame for the tail call, effectively replacing the current procedure activation with the new one, thus achieving the required constant stack space behaviour. If no tail call is pending, \texttt{executeProcedure} proceeds normally, calling the procedure's implementation (either \texttt{BuiltinProcedure::operator()} or \texttt{UserProcedure::executeBody}). \textbf{Only} when a procedure returns normally \textbf{without} setting up a tail call does the \texttt{executeProcedure} loop terminate and return a value.

\paragraph{Continuations}
Scheme's \texttt{call-with-current-continuation} (or \texttt{call/cc}) provides powerful control flow capabilities by reifying the current control context as a first-class procedure. Jaws implements this feature using exception handling to manage the non-local transfer of control.

The built-in procedure \texttt{jaws\_hof::callCC} takes a single argument, which must be a procedure (the 'receiver' procedure). When \texttt{callCC} is executed, it first captures the current state of the interpreter, including the active environment, into a \texttt{Continuation} object. This \texttt{Continuation} object itself derives from \texttt{Procedure}. \texttt{callCC} then immediately invokes the receiver procedure, passing the newly created \texttt{Continuation} object as its argument.

If the receiver procedure returns normally, its return value becomes the return value of the original \texttt{callCC} expression. However, if the \texttt{Continuation} object is ever invoked (called like a regular procedure), its \texttt{operator()} captures the value(s) passed to it and throws a special exception, \texttt{ContinuationInvocationException}. This exception carries both the captured \texttt{InterpreterState} from when the continuation was created and the value(s) passed to the continuation.

This exception propagates up the C++ call stack, unwinding it. The main interpretation loop contain a \texttt{try...catch} block specifically for \texttt{ContinuationInvocationException}. When caught, the interpreter state is completely replaced with the state stored inside the exception, effectively discarding the intermediate execution context and restoring the context from when \texttt{callCC} was originally invoked. The value stored in the exception then becomes the return value at the point where the continuation was restored. This allows Scheme code to implement complex control structures like non-local exits, coroutines, and backtracking.

\subsubsection{Quoting and Unquoting}

Scheme provides mechanisms to treat code as data (\texttt{quote}) and to selectively evaluate parts of data structures as code (\texttt{quasiquote}, \texttt{unquote}, \texttt{unquote-splicing}).

The \texttt{interpretQuote} function handles the \texttt{quote} special form. It simply prevents the evaluation of its operand expression. To return the expression structure as data, it relies on the helper function \texttt{expressionToValue}. This function recursively traverses the unevaluated AST \texttt{Expression} node provided to \texttt{quote}. Using \texttt{std::visit}, \texttt{expressionToValue} pattern matches on the type of the \texttt{Expression} node and constructs the corresponding runtime \texttt{SchemeValue} representation. For instance, an \texttt{AtomExpression} containing an identifier token becomes a \texttt{SchemeValue} holding a \texttt{Symbol}; an \texttt{sExpression} becomes a \texttt{SchemeValue} holding a \texttt{std::shared\_ptr<std::list<SchemeValue>>}, where each element of the C++ list is the result of recursively calling \texttt{expressionToValue} on the corresponding element in the AST \texttt{sExpression}. This systematic conversion transforms the AST representation of code into the first-class data structures (lists, symbols, numbers, etc.) manipulated by Scheme programs. For example, \texttt{(quote (+ 1 2))} results in a \texttt{SchemeValue} representing a list containing the symbol \texttt{+}, the number 1, and the number 2.

Quasiquotation, handled by \texttt{interpretQuasiQuote}, provides more intricate control over this code-to-data transformation. It works in conjunction with a recursive helper function, \texttt{processQuasiQuote}, which takes the expression and a nesting level parameter.

This mechanism allows programmers to construct complex data structures (often representing code templates) while selectively evaluating and inserting computed values or sequences.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% Start of Values Section (Revised)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%

\subsection{Values}

Scheme features a rich set of data types, and a compliant implementation must represent and manipulate these values correctly. In Jaws, all runtime values are uniformly represented by the \texttt{SchemeValue} class, which acts as a type-safe container for the various kinds of data that can exist during program execution.

Internally, \texttt{SchemeValue} utilizes the C++ \texttt{std::variant} feature (member \texttt{value}) to hold one of the possible underlying C++ types corresponding to Scheme types. This approach provides type safety at compile time while allowing a single \texttt{SchemeValue} object to represent any Scheme value at runtime. The \texttt{SchemeValue} class provides numerous predicate methods (e.g., \texttt{isNumber()}, \texttt{isList()}, \texttt{isProcedure()}, \texttt{isSymbol()}) to check the currently held type, as well as accessor methods (e.g., \texttt{asNumber()}, \texttt{asList()}) to retrieve the underlying data, often throwing an error if the type does not match expectations.

The primary Scheme data types specified by R7RS, along with Jaws-specific extensions, are represented within the \texttt{SchemeValue}'s variant as follows:
\begin{itemize}
    \item \textbf{Numbers:} Represented by the \texttt{Number} class, which itself uses a \texttt{std::variant} to implement Scheme's numeric tower, capable of holding integers (\texttt{int}), floating-point numbers (\texttt{double}), exact rationals (\texttt{Number::Rational}), and complex numbers (\texttt{std::complex<double>}).
    \item \textbf{Booleans:} Represented directly by C++ \texttt{bool} (\texttt{\#t} is \texttt{true}, \texttt{\#f} is \texttt{false}).
    \item \textbf{Pairs and Lists:} Represented by \texttt{std::shared\_ptr<std::list<SchemeValue>>}. A proper list is a chain of pairs terminating in the empty list. The empty list \texttt{()} is represented by a \texttt{SchemeValue} holding a pointer to an empty \texttt{std::list}. Pairs that do not form proper lists (dotted pairs) are also represented using this list structure convention where the final element is not the empty list.
    \item \textbf{Vectors:} Represented by \texttt{std::shared\_ptr<std::vector<SchemeValue>>}.
    \item \textbf{Strings:} Represented by \texttt{std::string}.
    \item \textbf{Characters:} Represented by \texttt{char}.
    \item \textbf{Symbols:} Represented by a simple \texttt{Symbol} struct containing the symbol's name as a \texttt{std::string}.
    \item \textbf{Procedures:} Represented by \texttt{std::shared\_ptr<Procedure>}, where \texttt{Procedure} is an abstract base class with derived classes \texttt{UserProcedure} (for lambda-defined procedures) and \texttt{BuiltinProcedure}.
    \item \textbf{Ports:} Represented by the \texttt{Port} class, which wraps various C++ stream types (\texttt{std::fstream}, \texttt{std::stringstream}, \texttt{SocketStream}) to handle file, string, and network I/O.
    \item \textbf{Continuations:} Represented by the \texttt{Continuation} class (derived from \texttt{Procedure}), capturing a saved interpreter state to enable non-local control flow via \texttt{call/cc}.
    \item \textbf{Threading Primitives:} Represented by \texttt{std::shared\_ptr} to \texttt{ThreadHandle}, \texttt{MutexHandle}, and \texttt{ConditionVarHandle} objects, providing mechanisms for concurrency control.
    \item \textbf{Multiple Values:} Represented by \texttt{std::shared\_ptr<MultiValue>} to handle functions returning multiple results, primarily used with \texttt{call-with-values}.
\end{itemize}

Operations on these values are implemented either directly on the C++ representation classes or via built-in procedures.

\subsection{Standard Library and Imports}

Following R7RS conventions and common Scheme practice, Jaws implements a significant portion of its standard functionality \textit{in Scheme itself}. This is achieved through a library system that allows Scheme code to be organized into modules, define exported bindings, and import functionality from other libraries. The core import mechanism is handled by C++ code, which then populates the interpreter's environment with the definitions provided by the Scheme libraries.

\subsubsection{Built-in Procedures}

The foundation of the Jaws Scheme environment is established by a set of built-in procedures implemented directly in C++. These procedures provide the core operations upon which the rest of the language, including much of the standard library, is built.

Built-ins are represented in the interpreter as \texttt{SchemeValue} objects containing a \texttt{std::shared\_ptr<BuiltinProcedure>}. The \texttt{BuiltinProcedure} class, derived from the base \texttt{Procedure} class, holds a callable C++ entity, typically a \texttt{std::function<std::optional<SchemeValue>(interpret::InterpreterState\&, const std::vector<SchemeValue>\&)>} (aliased as \texttt{BuiltInProcedure::Func}). This function pointer or lambda directly implements the primitive operation.

When the interpreter is initialized, the \texttt{createInterpreter} function populates the initial (root) environment by binding Scheme symbols (as \texttt{HygienicSyntax} objects) to instances of \texttt{BuiltinProcedure} wrapping the corresponding C++ implementation functions.

During execution, when \texttt{executeProcedure} encounters a \texttt{SchemeValue} whose underlying type is \texttt{BuiltinProcedure}, it directly invokes the stored C++ function (\texttt{func}), passing it the current \texttt{InterpreterState} and the vector of already-evaluated argument \texttt{SchemeValue}s. The C++ functions implementing these built-ins typically perform necessary argument count and type checking before operating directly on the underlying C++ data within the passed \texttt{SchemeValue} arguments or interacting with the interpreter state or external resources.

The implementations for the broad range of built-ins reside in the \texttt{builtins/} directory, covering areas such as: Mathematical operations (\texttt{JawsMath.cpp}), List/pair operations (\texttt{JawsList.cpp}), Input/Output (\texttt{JawsIO.cpp}), Equality/Type predicates (\texttt{JawsEq.cpp}), Higher-order functions (\texttt{JawsHof.cpp}), Threading (\texttt{JawsThread.cpp}), FFI (\texttt{jaws\_ffi.cpp}), and various value conversions and manipulations (\texttt{JawsVector.cpp}, \texttt{jaws\_string.cpp}, \texttt{jaws\_values.cpp}).

Notable among these are the threading and FFI implementations. The threading built-ins (\texttt{thread-spawn}, \texttt{mutex-create}, etc.) wrap standard C++ concurrency primitives to expose parallelism within Scheme. The FFI system (\texttt{load-library}, \texttt{register-function}) allows Jaws to interact with external C code by dynamically loading shared libraries and wrapping C functions.

\subsection{Standard Library and Imports} % Adjust section/subsection numbering as needed

Following R7RS conventions and common Scheme practice, Jaws implements a significant portion of its standard functionality \textit{in Scheme itself}. This is achieved through a library system that allows Scheme code to be organized into modules, define exported bindings, and import functionality from other libraries. The core import mechanism is handled by C++ code, which then populates the interpreter's environment with the definitions provided by the Scheme libraries.

\paragraph{Import Mechanism}
The loading and processing of libraries are managed by functions within the \texttt{import} namespace (\texttt{Import.cpp}). When the interpreter starts or encounters an \texttt{import} declaration, the \texttt{processImports} function identifies required libraries. It resolves library names (e.g., \texttt{(jaws list-utils)}) to file paths via \texttt{resolveLibraryPath} (handling platform differences) and invokes \texttt{importLibrary}. This function reads and parses the \texttt{.scm} file, expecting a \texttt{(define-library ...)} form, extracts exports and body expressions, checks for circular dependencies, and stores the resulting \texttt{LibraryData} in a global \texttt{LibraryRegistry} to prevent redundant loads. The \texttt{preloadLibraries} function can load libraries from a base directory at startup. Finally, \texttt{populateInterpreterStateFromRegistry} and \texttt{populateMacroEnvironmentFromRegistry} use the registry to make the exported values and syntax definitions available to the interpreter and macro expander, respectively, interpreting definitions as needed.

\paragraph{Standard Library Content}
The standard libraries, located in the \texttt{lib/} directory, are implemented in Scheme and provide much of the language's user-facing functionality. Key libraries like \texttt{(base)}, \texttt{(list-utils)}, \texttt{(loops)}, \texttt{(utilities)}, \texttt{(vector-utils)}, and \texttt{(math)} define fundamental macros (\texttt{cond}, \texttt{let*}, \texttt{and}, \texttt{or}, various loops), common list and vector utilities, higher-order functions (\texttt{map}, \texttt{filter}, \texttt{fold-left}), and derived mathematical procedures. The successful implementation and execution of these libraries demonstrate that the Jaws interpreter core provides a sufficiently powerful foundation to run substantial Scheme programs. Further details on the specific functions provided by each library can be found in the source code located in the appendix or supplementary material.

%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% End of Standard Library and Imports Section (Condensed)
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%
% End of Standard Library and Imports Section
%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%





Could be a section each for implementation and evaluation if this suits you better or you could use subsections instead. The difference between this section and the previous "Methodology" section is that this one covers "action" or in other words your active contributions to the project. These may include:
\begin{itemize}
\item Implementation of programming code: Describe your final code architecture using for example (UML) diagrams and code snippets. Make sure that code snippet (figure) captions are self-explanatory which means that you should not have to consult the text body to understand what is shown in the figure. Many code snippets of the same kind should end up in an appendix instead.
\item Results from experiments run, including testing (user and software). Use figures and tables with self-explanatory captions (see earlier statement). Multiple figures and tables that cover several pages should be put in an appendix.
\item Analysis of results: Discuss your experimental and/or test findings in depth. Compare them against other studies and/or benchmarks, and point out limitations of your work (that could be due to limited time) and elaborate on scope for improvement.
\end{itemize}

\section{Conclusion and Future Work}

Another essential section that should keep its title as suggested. Briefly discuss your main findings, outcomes, results; what worked and what could have been done differently. Then summarise your work in a concluding statement by comparing your outcomes against the main and sub-objectives and/or MoSCoW requirements (if used) and suggest potential future work that could be done if more time would be available.

\clearpage

\bibliography{reportbib}

\appendix
\clearpage
\section{Lexer Implementation Details} \label{app:regex}

\subsection{Regular Expression Patterns}
The lexer for the Scheme interpreter utilises the following regular expression patterns to identify tokens:

\begin{table}[h]
\caption{Regular Expression Patterns for Scheme Lexer}
\label{tab:regexpatterns}
\begin{tabular}{|l|p{10cm}|}
\hline
\textbf{Token Type} & \textbf{Regex Pattern} \\
\hline
COMMENT & comments \\
\hline
STRING & Pattern for string literals \\
\hline
COMPLEX & Pattern for complex numbers \\
\hline
RATIONAL & Pattern for rational numbers \\
\hline
FLOAT & Pattern for floating-point numbers \\
\hline
INTEGER & Pattern for integers \\
\hline
BOOLEAN & Patterns for true and false values \\
\hline
IDENTIFIER & Pattern for identifiers \\
\hline
PARENTHESES & Patterns for opening and closing parentheses \\
\hline
SPECIAL SYMBOLS & Patterns for quote, dot, and other special symbols \\
\hline
WHITESPACE & Pattern for whitespace characters \\
\hline
\end{tabular}
\end{table}

\subsection{Tokenization Algorithm} \label{app:tokenize}
\begin{algorithm}
\caption{Tokenization Algorithm for Scheme}
\label{alg:tokenize}
\begin{algorithmic}[1]
\Procedure{Tokenize}{input}
    \State Initialize token collection and state
    \While{not at end of input}
        \State Find next matching pattern
        \If{no match found}
            \State Report error for unexpected character
        \Else
            \State Create token from matched text
            \If{token is not whitespace or comment}
                \State Add to token collection
            \EndIf
            \State Update position in input
        \EndIf
    \EndWhile
    \State Add end-of-file token
    \State \Return token collection
\EndProcedure
\end{algorithmic}
\end{algorithm}
\subsection{Trampolining Algorithm} \label{app:trampoline}

\begin{algorithm}
\caption{Trampolining Algorithm for Tail Call Optimisation}
\label{alg:trampoline}
\begin{algorithmic}[1]
\Procedure{ExecuteProcedure}{state, procedure, arguments}
\State currentProc $\gets$ procedure
\State currentArgs $\gets$ arguments
\While{true}
    \While{currentProc is TailCall}
        \State currentProc $\gets$ tailCall.proc
        \State currentArgs $\gets$ tailCall.args
    \EndWhile
    
    \State result $\gets$ currentProc(state, currentArgs)
    
    \If{result is not TailCall}
        \State \Return result
    \Else
        \State currentProc $\gets$ result
    \EndIf
\EndWhile
\EndProcedure
\Procedure{InterpretTailCall}{state, expression}
\If{expression is a procedure call}
\State Create TailCall object
\State \Return ExecuteProcedure with TailCall
\Else
\State \Return normal interpretation
\EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}

\begin{algorithm}
\caption{Recursive Descent Parser for Scheme}
\label{alg:recursive-descent}
\begin{algorithmic}[1]
\Procedure{Parse}{tokens}
    \State \Return \Call{ParseExpression}{}
\EndProcedure

\Procedure{ParseExpression}{}
    \State token $\gets$ current token
    
    \If{token is LEFT\_PAREN}
        \State Consume LEFT\_PAREN
        \State expressions $\gets$ Parse all expressions until RIGHT\_PAREN
        \State Consume RIGHT\_PAREN
        
        \If{expressions is empty}
            \State \Return new EmptyList()
        \ElsIf{first expression is "define"}
            \If{second expression is identifier}
                \State \Return new Definition(name, value)
            \Else
                \State \Return new FunctionDefinition(name, params, body)
            \EndIf
        \ElsIf{first expression is "let"}
            \State \Return new LetExpression(bindings, body)
        \Else
            \State \Return new FunctionApplication(operator, arguments)
        \EndIf
        
    \ElsIf{token is QUOTE}
        \State Advance token
        \State \Return new QuoteExpression(\Call{ParseExpression}{})
    \ElsIf{token is literal}
        \State Advance token
        \State \Return new LiteralExpression(token.value)
    \ElsIf{token is IDENTIFIER}
        \State Advance token
        \State \Return new VariableExpression(token.lexeme)
    \EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}
\begin{algorithm}
\caption{Dead Code Elimination using Dependency Graph}
\label{alg:dce}
\begin{algorithmic}[1]
\Procedure{DeadCodeElimination}{$Program$}
    \State $G \gets \text{ConstructDependencyGraph}(Program)$
    \State $Roots \gets \text{GetRootNodes}(G)$ \Comment{Nodes with side effects or program outputs}
    \State $LiveNodes \gets \emptyset$
    \State $WorkList \gets Roots$
    
    \While{$WorkList \neq \emptyset$}
        \State $node \gets \text{Dequeue}(WorkList)$
        \State $LiveNodes \gets LiveNodes \cup \{node\}$
        \For{each $dependent$ in $\text{GetDependents}(G, node)$}
            \If{$dependent \notin LiveNodes$}
                \State $\text{Enqueue}(WorkList, dependent)$
            \EndIf
        \EndFor
    \EndWhile
    
    \State $Program' \gets \text{RemoveDeadBindings}(Program, LiveNodes)$
    \State \Return $Program'$
\EndProcedure
\end{algorithmic}
\end{algorithm}
\begin{algorithm}
\caption{Constant Folding for ANF Expressions}
\label{alg:const-fold}
\begin{algorithmic}[1]
\Procedure{ConstantFolding}{$Program$}
    \State $ConstMap \gets \emptyset$
    \State $FoldedProgram \gets \emptyset$
    
    \For{each binding $(var, expr)$ in $Program$}
        \State $foldedExpr \gets \text{FoldExpression}(expr, ConstMap)$
        
        \If{$\text{IsLiteral}(foldedExpr)$}
            \State $ConstMap[var] \gets foldedExpr$
        \EndIf
        
        \State Add $(var, foldedExpr)$ to $FoldedProgram$
    \EndFor
    
    \State \Return $FoldedProgram$
\EndProcedure

\Procedure{FoldExpression}{$expr, ConstMap$}
    \If{$expr$ is variable and $expr$ in $ConstMap$}
        \State \Return $ConstMap[expr]$
        
    \ElsIf{$expr$ is primitive operation}
        \State $args \gets$ empty list
        \State $allConstant \gets \text{true}$
        
        \For{each $arg$ in $expr.args$}
            \State $foldedArg \gets \text{FoldExpression}(arg, ConstMap)$
            \State Add $foldedArg$ to $args$
            \If{not $\text{IsLiteral}(foldedArg)$}
                \State $allConstant \gets \text{false}$
            \EndIf
        \EndFor
        
        \If{$allConstant$ and $\text{IsPure}(expr.op)$}
            \State \Return $\text{Evaluate}(expr.op, args)$
        \Else
            \State \Return new Operation($expr.op, args$)
        \EndIf
        
    \Else
        \State \Return $expr$
    \EndIf
\EndProcedure
\end{algorithmic}
\end{algorithm}
\end{document}

